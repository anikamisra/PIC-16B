[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/hw-0/index.html",
    "href": "posts/hw-0/index.html",
    "title": "HW0",
    "section": "",
    "text": "In today’s dataset from Github, I will be explaining how to create a simple visualization from the Palmer Penguins dataset. There are many features in this dataset, such as species, region, island, egg stage, culmen features, body mass, gender, and delta 15 N features. However, the question we are trying to answer today is:\n\n\nLet’s get started."
  },
  {
    "objectID": "posts/hw-0/index.html#improvements-and-next-steps",
    "href": "posts/hw-0/index.html#improvements-and-next-steps",
    "title": "HW0",
    "section": "Improvements and next steps:",
    "text": "Improvements and next steps:\n\ncreate DataFrames of each island and species to determine the exact range of body masses\nGet more specific statistics and exact numbers as opposed to relying on visualizations\nGet more data from Chinstrap and Gentoo penguins that comes from other islands\n\nI hope you learned something with today’s blog post! Have an ice day.\n\n\n\nHave an ice day.png\n\n\nImage sources: Ice\nPenguin"
  },
  {
    "objectID": "posts/hw-1/index.html",
    "href": "posts/hw-1/index.html",
    "title": "Using SQL Querying to compare different countries’ climate trends and also to compute year-long trends of temperature increases",
    "section": "",
    "text": "Welcome!\nToday we will be creating several interactive plots from the NOAA climate dataset. However, because this dataset is extremely large, we will be taking advantage of SQL querying, which allows us to select the data we need without uploading the entire file and using up too much memory.\n\n\nSection 1: Creating the database\nFirst, we will upload all necessary packages.\n\nimport sqlite3\nimport pandas as pd\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nNow let’s enter the relevant file names.\n\ncountries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ntemps_url = \"temps.csv\"\nstation_url = \"station-metadata.csv\"\n\n# add link to download other tables \n\nNow we will create a database called my_database.db.\n\nconn = sqlite3.connect(\"database.db\") # this creates a database in current directory called my_database.db\n\nNow we are ready to add our first table to our database. The entire purpose of using SQL querying is that we avoid uploading the entire table in order to prevent overusage of RAM. Because the temperatures dataset is extremely large, for this reason, we will upload it chunk by chunk. And, because the other datasets are smaller, we can just upload them all at once.\nLet’s set the chunksize of the temps dataframe to 100,000. With the chunksize keyword specified, the read_csv function of pandas creates an iterator of the temps dataframe.\n\nCHUNKSIZE=100000\ntemps_df_iter = pd.read_csv(temps_url, chunksize=CHUNKSIZE)\ntemps_df = temps_df_iter.__next__()\ntemps_df.shape\n\n(100000, 14)\n\n\nAs we can see, the temps dataframe indeed has only 100,000 rows! Let’s finish uploading the other files and then preview everything.\n\ncountries_df = pd.read_csv(countries_url)\ncountries_df.shape\n\n(279, 3)\n\n\n\nstations_df = pd.read_csv(station_url) \nstations_df.shape\n\n(27585, 5)\n\n\nAs we can see, the stations and countries files are not nearly as large as the temperatures file. Now, let’s preview what each table looks like so we are familiar with the column names and how to query accordingly.\n\ntemps_df.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\n\ncountries_df.head()\n\n\n\n\n\n\n\n\nFIPS 10-4\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa\n\n\n\n\n\n\n\n\nstations_df.head()\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\n\n\n\n\n\n\n\nGreat! We are finally ready to add tables to our database. We will add three tables, one for temperatures, one for stations, and one for countries.\nFirst, let’s create a function to clean up the temperatures dataframe a bit.\n\ndef prepare_df(df):\n    \"\"\"\n    Function that cleans and reorganizes the temperatures dataframe. \n    \n    args: \n        df (pandas.DataFrame): the dataframe to organize, from the temperatures file of the NOAA database \n    returns: \n        df (pandas.DataFrame): the reorganized dataframe \n    \"\"\"\n    \n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\nNow let’s add the temperatures dataframe to our SQL database as a table by working with only 100,000 rows at a time. We are able to do this thanks to the iterator we created.\n\nfor i, df in enumerate(temps_df_iter):\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\nNow, let’s add the other two dataframes as tables to our database.\n\nstations_df.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\ncountries_df.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n279\n\n\nLet’s make sure that everything was added to our database successfully. Running the following code will allow us to see what is currently in our database.\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\nAwesome! We have three tables, one for temps, one for stations, and one for countries. Now, we are done with our database connection, so it is good practice to close it.\n\nconn.close()\n\n\n\nPart 2: Querying our first database\nLet’s now use our database to perform an SQL query that returns a pandas dataframe. The beauty of this is that we do not have to have the entire dataframe in our RAM. To do this, we will use the query_climate_database() function which I have already defined. Let’s see what it looks like below:\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month): \n    \"\"\"\n    Function which performs SQL query based on country, year beginning, year end, and month. \n    \n    args: \n        db_file (str): string for database name \n        country (str): string for country \n        year_begin (int): year in XXXX format for which year you want to start from \n        year_end (int): year in XXXX format for which year you want to end from \n        month (int): integer value of the month you are trying to query \n    returns: \n        df (pandas.DataFrame): a dataframe with these selected values \n    \"\"\"\n    \n    # connect database \n    conn = sqlite3.connect(db_file) \n    \n    # perform query\n    query = f\"\"\"\n    SELECT \n        temperatures.Year, \n        temperatures.Month, \n        temperatures.Temp, \n        stations.NAME, \n        stations.LATITUDE, \n        stations.LONGITUDE, \n        countries.Name \n    FROM \n        temperatures\n    JOIN \n        stations ON temperatures.ID = stations.ID\n    JOIN \n        countries ON substr(temperatures.ID, 1, 2) = countries.[FIPS 10-4]\n    WHERE \n        countries.Name = '{country}'\n        AND temperatures.Year BETWEEN {year_begin} AND {year_end}\n        AND temperatures.Month = {month}\n    \"\"\"\n    # convert query into dataframe \n    df = pd.read_sql_query(query, conn)\n    df = df.reindex(columns=['NAME', 'LATITUDE', 'LONGITUDE', 'Name', 'Year', 'Month', 'Temp'])\n    df.columns = ['NAME', 'LATITUDE', 'LONGITUDE', 'Country', 'Year', 'Month', 'Temp']\n    \n    # close connection \n    conn.close()\n    \n    return df \n\n\n\nSo this function takes 5 arguments 1. Database name 2. Country name 3. Beginning year 4. Ending year 5. Month number\nLet’s test it out with the code below:\n\nquery_climate_database(db_file = \"database.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns\n\n\n\n\n\nPart 3: Geographic scatterplots\nNow, let’s use SQL querying to create some geographic scatterplots with the NOAA data. First, let’s take a look at the function which will plot this figure for us:\n\nfrom climate_database import temperature_coefficient_plot\nprint(inspect.getsource(temperature_coefficient_plot))\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n     Function which creates a geographic scatterplot to see how yearly temperature increase vary across different locations (stations) for a given timeframe for a certain country. \n    \n    args: \n        db_file (str): string for database name \n        country (str): string for country to be plotted \n        year_begin (int): year in XXXX format for which year you want to start from \n        year_end (int): year in XXXX format for which year you want to end from\n        month (int): integer value of the month you are trying to query\n        min_obs (int): the minimum observations per data point plotted \n        **kwargs: arbitrary keyword arguments for the plot \n        \n    returns: \n        fig (plotly.express.scatter_mapbox): A geographic scatterplot with yearly temperature increase at different stations.\n    \"\"\"\n    # query climate database\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    # use transform function here to filter out the stations that don't have at least min_obs years worth of data \n    \n    # calculate yearly change in temperature\n    df = df[df.groupby('NAME')['Year'].transform('count') &gt;= min_obs] \n    \n    coefs = df.groupby('NAME').apply(coef).reset_index()\n    coefs.columns = ['NAME', 'YearlyChange']\n    \n    df = pd.merge(df, coefs, on='NAME')\n    \n    #df['YearlyChange'] = df['YearlyChange'].round(4)\n    \n    # create the scatterplot\n    fig = px.scatter_mapbox(df, lat='LATITUDE', lon='LONGITUDE', color='YearlyChange',\n                            color_continuous_midpoint=0, # centers the colorbar at 0 \n                            hover_data={'NAME': True, 'YearlyChange': ':.3f'}, # shows 3 digits after the decimal point \n                            labels={'YearlyChange': 'Estimated Yearly Increase (°C)'}, \n                            title=f\"Estimates of yearly increase in temperature in {pd.to_datetime(month, format='%m').month_name()} for stations in {country}, years {year_begin}-{year_end}\",  \n                            **kwargs)\n    \n    return fig\n\n\n\nAs we can see, the temperature_coefficient_plot function takes in 6 or more arguments: 1. database name 2. country name 3. beginning year 4. end year 5. month number 6. minimum observations in data point 7. additional keyword arguments for graph\nIt uses the first 5 arguments as input in the query_climate_database we tested earlier. Using this information, let’s create a geographic scatterplot of the yearly increase in temperature in January for India from 1980 to 2020.\n\nimport plotly.express as px\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"database.db\", \"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nInteresting! The estimated yearly increase actually appears to be decreasing near the Northeast region next to the Himalayas. I guess that makes sense, seeing how cold those mountains are. Let’s use the same function to see how this compares to Russia.\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"database.db\", \"Russia\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nCool! Russia’s temperatures appear to be decreasing in the same time period in several places as well.\n\n\nPart 4: Other types of Interactive Plots\nLet’s create a new query function that does not depend on the month. We can use this to create other kinds of interactive plots with plotly, that will answer two important questions:\nQuestion 1: How does the average yearly change in temperature vary based on elevation when comparing 2 countries in one timeframe?\nQuestion 2: How does the month impact the yearly increase in temperature for certain years in a given country?\nLet’s get started. First, let’s take a look at what the new query function looks like:\n\nfrom climate_database import query_climate_database_2\nprint(inspect.getsource(query_climate_database_2))\n\ndef query_climate_database_2(db_file, country, year_begin, year_end): \n    \"\"\"\n    Function which performs SQL query based on country, year beginning, and year end. \n    \n    args: \n        db_file (str): string for database name \n        country (str): string for country \n        year_begin (int): year in XXXX format for which year you want to start from \n        year_end (int): year in XXXX format for which year you want to end from \n    returns: \n        df (pandas.DataFrame): a dataframe with these selected values \n    \"\"\"\n    \n    # connect database \n    conn = sqlite3.connect(db_file) \n    \n    # perform sql query \n    query = f\"\"\"\n    SELECT \n        temperatures.Year, \n        temperatures.Month,  \n        temperatures.Temp, \n        stations.NAME, \n        stations.LATITUDE, \n        stations.LONGITUDE, \n        stations.STNELEV,\n        countries.Name\n    FROM \n        temperatures \n    JOIN \n        stations ON temperatures.ID = stations.ID\n    JOIN \n        countries ON substr(temperatures.ID, 1, 2) = countries.[FIPS 10-4]\n    WHERE \n        countries.Name = '{country}'\n        AND temperatures.Year BETWEEN {year_begin} AND {year_end}\n    \"\"\"\n    \n    # convert query into dataframe \n    df = pd.read_sql_query(query, conn) \n    \n    # close connection \n    conn.close() \n    return df\n\n\n\nAs we can see, this query function is the same as the previous one except it does not take in month. We can use this query function to create pandas dataframes that are more generalized, and then create plots to let us see more longterm trends. Let’s start with question 1, which is simple to plot but interesting to know.\n\nQuestion 1: How does the average yearly change in temperature vary based on elevation when comparing 2 countries in a given timeframe?\nTo answer this question, we will use the elevation_coefficient_plot() function. We will create a multifaceted, interactive plot. Let’s inspect this function to see what it looks like:\n\nfrom climate_database import elevation_coefficient_plot\nprint(inspect.getsource(elevation_coefficient_plot))\n\ndef elevation_coefficient_plot(db_file, country1, country2, year_begin, year_end, **kwargs): \n    \"\"\"\n     Function which creates an interactive plot for elevation vs. yearly increase in temperature for a given timeframe for two countries. \n    \n    args: \n        db_file (str): string for database name \n        country (str): string for country to be plotted \n        year_begin (int): year in XXXX format for which year you want to start from \n        year_end (int): year in XXXX format for which year you want to end from\n        **kwargs: arbitrary keyword arguments for the plot \n        \n    returns: \n        fig (plotly.express.scatter): A scatter plot of elevation on the x-variable and yearly change on the y-variable.  \n    \"\"\"\n    \n    # obtain one dataframe for each country \n    df1 = query_climate_database_2(db_file, country1, year_begin, year_end)\n    df2 = query_climate_database_2(db_file, country2, year_begin, year_end)\n    \n    # calculate coefs for each country \n    coefs1 = df1.groupby('NAME').apply(coef).reset_index()\n    coefs2 = df2.groupby('NAME').apply(coef).reset_index()\n    coefs1.columns = ['NAME','YearlyChange']\n    coefs1['Country'] = country1\n    coefs2.columns = ['NAME', 'YearlyChange']\n    coefs2['Country'] = country2\n    \n    # combine all findings \n    coefs_combined = pd.concat([coefs1, coefs2])\n    df1 = pd.merge(df1, coefs1, on='NAME') \n    df2 = pd.merge(df2, coefs2, on='NAME')\n    df_combined = pd.concat([df1, df2])\n    \n    # plot multifaceted figure \n    fig = px.scatter(df_combined, x='STNELEV',\n                     y='YearlyChange',\n                     hover_data={'NAME': True, 'LATITUDE': ':.3f', 'LONGITUDE': ':.3f', 'YearlyChange': ':.3f'},\n                     labels={'STNELEV': 'Station Elevation (meters)', 'YearlyChange': 'Yearly Temperature Change (°C)'},\n                     title=f'Correlation between Elevation and Yearly Increase in Temperature in {country1} and {country2} for the years {year_begin}-{year_end}', \n                     facet_col='Country',\n                     **kwargs)\n    return fig \n\n\n\nAs we can see, this function takes in the same arguments as query_climate_database_2, except it takes in two countries instead of 1, and also provides room for one more optional argument for additional keyword arguments. The way this function works is: 1. It first performs the query_climate_database_2 function to obtain the necessary data for country 1. 2. Repeat for country 2. 2. Then, it creates a combined dataframe for this data. 3. Finally, creates an interactive plotly scatterplot that allows us to see the correlation between elevation and average yearly change in temperature for these two countries.\nNow, let’s compare India vs China from 1990 to 2000.\n\nfig = elevation_coefficient_plot(\"database.db\", \"India\", \"China\", 1990, 2000)\nfig.show()\n\n\n\n\nWow! As we can see, each value is nicely rounded, and we can hover over each datapoint to learn more about it, such as the station name, the exact yearly temperature change, and more. However, there does not appear to be much correlation at all. So, to answer our question:\nIn both India and China, from 1990-2000, the average yearly change in temperature does not depend on elevation.\nHowever, we can see that China has more stations total, and these statiosn have higher elevations. This is one of the advantages of using multifaceted plots.\nWe can always make more scatterplots to see if there is any correlation in other countries!\nNow, however, we want to answer the question:\n\n\nQuestion 2: How does the month impact the yearly increase in temperature for certain years in a given country?\nTo do this, we will use the same query_climate_database_2 function, but this time, use the temperature_increase_boxplot function to graph it. Let’s, once again, take a look at what this function looks like.\n\nfrom climate_database import temperature_increase_boxplot\nprint(inspect.getsource(temperature_increase_boxplot))\n\ndef temperature_increase_boxplot(db_file, country, year_begin, year_end, **kwargs): \n    \"\"\"\n     Function which creates an interactive boxplot to see how yearly temperature increase varies across the months, for a given timeframe for a certain country. \n    \n    args: \n        db_file (str): string for database name \n        country (str): string for country to be plotted \n        year_begin (int): year in XXXX format for which year you want to start from \n        year_end (int): year in XXXX format for which year you want to end from\n        **kwargs: arbitrary keyword arguments for the plot \n        \n    returns: \n        fig (plotly.express.box): A boxplot of yearly temperature increase over the months.\n    \"\"\"\n    # perform sql query \n    df = query_climate_database_2(db_file, country, year_begin, year_end)\n\n    # find coefs for dataframe \n    coefs = df.groupby(['NAME', 'Month']).apply(coef).reset_index()\n    coefs.columns = ['NAME', 'Month', 'YearlyChange']\n    \n    # clean dataframe \n    df = pd.merge(df, coefs, on=['NAME', 'Month'])\n    df['Month'] = pd.to_datetime(df['Month'], format='%m').dt.month_name()\n    df['YearlyChange'] = df['YearlyChange'].round(3)\n    \n    # Create the boxplot\n    fig = px.box(df, x='Month', y='YearlyChange',\n                 color = 'Month',\n                 labels={'Month': 'Month', 'YearlyChange': 'Yearly Temperature Change (°C)'},\n                 title=f'Month-by-Month Statistics of Yearly Increase in Temperature in {country} from {year_begin}-{year_end}',\n                 hover_data={'YearlyChange': ':.3f'},\n                 **kwargs)\n    \n    return fig\n\n\n\ntemperature_increase_boxplot takes in the same arguments as elevation_coefficient_plot, except this time, it makes a box plot based on every month of the year. Let’s see this in action.\n\nfig = temperature_increase_boxplot(\"database.db\", \"India\", 1980, 2020)\n\nfig.show()\n\n\n\n\nWow, what an informative plot! The axes are nicely labelled, and, if we look closely, we can see a monthly pattern in yearly increase in temperature. It appears to be going in a slight wave up and down. However, the data points do have a lot of outliers.\n\n\n\nPart 5: Summary and takeaways\nToday, we learned how to use SQL querying to create pandas dataframes and create interactive plotly plots. We answered specific questions such as how to compare 2 different countries’ climate trends, and also how to view long-term yearly trends. SQL can open up many doors because we are not limited by our computer’s RAM. I hope you learned something from this post, and have a fantastic day!"
  },
  {
    "objectID": "posts/hw-4/index.html",
    "href": "posts/hw-4/index.html",
    "title": "Optimizing 2D heat diffusion by experimenting with matrix-vector multiplication, JAX, Numpy, and more.",
    "section": "",
    "text": "Today we will be simulating 2D heat diffusion in various ways. We will be experimenting with four different functions that are predefined in my heat_equation.py file. I will show what each function looks like and explain how it works before viewing the results.\nThe first step is to make sure to import all the necessary packages. There are a lot!\n\nimport jax\nimport jax.numpy as jnp\nimport time\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom jax.experimental import sparse\nimport inspect\n\nAlright, let’s go ahead and dive right in."
  },
  {
    "objectID": "posts/hw-4/index.html#welcome",
    "href": "posts/hw-4/index.html#welcome",
    "title": "Optimizing 2D heat diffusion by experimenting with matrix-vector multiplication, JAX, Numpy, and more.",
    "section": "",
    "text": "Today we will be simulating 2D heat diffusion in various ways. We will be experimenting with four different functions that are predefined in my heat_equation.py file. I will show what each function looks like and explain how it works before viewing the results.\nThe first step is to make sure to import all the necessary packages. There are a lot!\n\nimport jax\nimport jax.numpy as jnp\nimport time\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom jax.experimental import sparse\nimport inspect\n\nAlright, let’s go ahead and dive right in."
  },
  {
    "objectID": "posts/hw-4/index.html#part-2-matrix-vector-mulitplication-with-jax-and-sparse-matrices",
    "href": "posts/hw-4/index.html#part-2-matrix-vector-mulitplication-with-jax-and-sparse-matrices",
    "title": "Optimizing 2D heat diffusion by experimenting with matrix-vector multiplication, JAX, Numpy, and more.",
    "section": "Part 2: Matrix-vector mulitplication with JAX and sparse matrices",
    "text": "Part 2: Matrix-vector mulitplication with JAX and sparse matrices\nOne of the issues with using A in Part 1 is that we waste a lot of time computing the 0s during the matrix vector multiplication. Luckily, Jax has a solution for this called the batched coordinate format (BCOO), which exploits the 0s and increases computing time. Saving space on the 0’s is what turns A into a “sparse” matrix. If we do not use Jax’s batched coordinate format, then A is a “dense” matrix. In order to utilize it, let’s turn A into a “sparse” matrix. Here is how we can do this:\n\nfrom heat_equation import get_sparse_A\nprint(inspect.getsource(get_sparse_A))\n\ndef get_sparse_A(N): \n    \"\"\"\n    Creates finite difference matrix that is sparse. \n    Args:\n        N: the square root of the row and column sizes of A \n    Returns: \n        A: the N^2 x N^2 finite difference matrix A that is in sparse format \n    \"\"\"\n    n = N * N\n    # create proper entries for 2D heat diffusion in matrix-vector multiplication\n    # utilize jax.numpy in order to be compatible with jax's XLA compiler  \n    diagonals = [-4 * jnp.ones(n), jnp.ones(n-1), jnp.ones(n-1), jnp.ones(n-N), jnp.ones(n-N)] \n    \n    # make the other rows and columns 0 \n    diagonals[1] = diagonals[1].at[(N-1)::N].set(0)\n    diagonals[2] = diagonals[2].at[(N-1)::N].set(0)\n    \n    # create A \n    A = jnp.diag(diagonals[0]) + jnp.diag(diagonals[1], 1) + jnp.diag(diagonals[2], -1) + jnp.diag(diagonals[3], N) + jnp.diag(diagonals[4], -N)\n    \n    # use batched coordinate format from Jax to use less space in the matrix \n    A_sp_matrix = sparse.BCOO.fromdense(A)\n    return A_sp_matrix\n\n\n\nRemember the “@jax.jit” decorator at the top of our advance_time_matvecmul() function? We want the matrix multiplier to actually utilize jax’s XLA compiler, aka, Accelerated Linear Algebra.\nSo, we are using the “jit-ed” version of advance_time_matvecmul(). In order to do this, we need to also use jax.numpy operations when we create the sparse version of A.\nFinally, we convert A to a sparse matrix in order to save space on the 0s. This is the matrix we pass in to advance_time_matvecmul() when we run our heat simulation diffusion.\nLet’s go ahead and run this simulation using the sparse matrix, and see if it improves the speed.\n\n# redefine u0 (since it was used in our last simulation) \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n# get sparse matrix format of A \nA_sparse = get_sparse_A(N)\n\n# run the simulation \nsolutions = dict()\nstart_time = time.time()\n\nfig, axs = plt.subplots(3,3)\nfor i in range(2700): \n    u0 = advance_time_matvecmul(A_sparse, u0, epsilon) \n    # store immediate solution \n    solutions[i] = u0 \nend_time = time.time()\n# calculate total time for simulation, not including visualization \ntotal_time = end_time - start_time\n\n# visualize \nfor i in range(2700): \n    if (i + 1) % 300 == 0: \n        ax = axs[i // 300 // 3, i // 300 % 3]\n        ax.imshow(solutions[i])\n        ax.set_title(f'Iteration {i+1}')\n\nprint(f\"time it took: {total_time:0.2f} sec\")\n\ntime it took: 0.75 sec\n\n\n\n\n\n\n\n\n\nWow! We jumped from 168 seconds to 0.75 seconds. That’s almost 230x faster! Talk about an improvement. Looks like utilizing jax’s BCOO format, along with utilizing its XLA compiler really speeds things up. This is why everyone loves GPUs. But what if we don’t even need a finite difference matrix, A? That brings us to the next part…."
  },
  {
    "objectID": "posts/hw-2/index.html",
    "href": "posts/hw-2/index.html",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "",
    "text": "Welcome! In today’s post, I will be describing how to build a webscraper to scrape the TMDB movie database. By the end of this post, you should understand how to build a webscraper, and you should be able to use this webscraper for the movie of your choice to find recommendations based on it. For my movie, I’m going to choose Legally Blonde.\n\n\n\nSegmentLocal\n\n\nImage source\nWhat a girlboss!!"
  },
  {
    "objectID": "posts/hw-2/index.html#part-0a-initializer",
    "href": "posts/hw-2/index.html#part-0a-initializer",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 0A: Initializer",
    "text": "Part 0A: Initializer\nJust like Elle Woods had to study hard for her LSAT before she got into Harvard, we all have to start somewhere. Before webscraping the TMDB website, we have to build our scraper. In this section, I will explain how to build the scraper and how the scraper works. Then, in part B, I will explain where this TmdbSpider class should go and how to actually set up the scraper in your local computer. So for now, just follow along with the explanation, and we will set everything up in Part B.\nWe begin this webscraper by importing the necessary packages, defining a class, and constructing the initializer.\nWe will name the class TmdbSpider since we are creating a webscraper for the TMDB database. It will inherit the class scrapy.Spider.\nIn the initializer, we will accept a “subdir” string argument. This argument is a string that contains the last part of the url the movie we want to scrape. It should look something like this:\n“8835-legally-blonde”\nThis url ending can be found by looking at the url of the movie we want to scrape on the TMDB website.\nThen, we will create an instance variable called start_urls that builds the complete url for the movie to scrape.\n\n# import necessary packages \nimport scrapy \n\n# define a class that inherits scrapy.Spider, the base class for spiders \nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider' # name our spider \n    def __init__(self, subdir=None, *args, **kwargs):\n        \"\"\" \n        Class constructor for the TmdbSpider class. \n        \n        Args: \n            self (TmdbSpider class instance): instance of the TmdbSpider class. \n            subdir (string): String for the subdirectory of the movie we want to start with. This string can be found in the movie url \n            *args: additional arbitrary keyword arguments \n            **kwargs: additional keyword arguments  \n            \n        \"\"\" \n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"] # build the complete url from the subdirectory"
  },
  {
    "objectID": "posts/hw-2/index.html#part-1a-parse-method",
    "href": "posts/hw-2/index.html#part-1a-parse-method",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 1A: Parse Method",
    "text": "Part 1A: Parse Method\nThe first class method we will define is the parse() function. This is how the parse() function works:\n\nFirst, obtain the url of the full cast and crew. This will be done by “hardcoding”, aka, manually adding a “cast” string onto our start_urls instance variable.\nCall the parse_full_credits() method. This will be done by specifying a callback argument to a yielded scrapy request.\n\nThis method assumes we start on a movie page, and it should navigate us to a “full cast and crew” page for that specific movie. Then, it calls the next parse function, which we will define in part 2.\n\ndef parse(self, response): \n        \"\"\"\n        Assumptions: \n            Assume we are starting on a movie page. \n        Effects: \n            Navigate to a \"full cast and crew\" page for that specific movie. \n            Data outputs: A yielded scrapy request calling parse_full_credits() method with the harcoded \"full cast and crew\" url \n        \"\"\"\n        \n        # first, obtain the url of full cast and crew (hardcoded)\n        # response.url is a built-in method that gives us current url of response\n        cast_url = response.url + '/cast/'  \n          \n        \n        # parse_full_credits method is called by specifying callback argument to yielded scrapy request\n        yield scrapy.Request(url = cast_url, callback = self.parse_full_credits)"
  },
  {
    "objectID": "posts/hw-2/index.html#important",
    "href": "posts/hw-2/index.html#important",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Important:",
    "text": "Important:\nPlease note that each of the parse methods we are defining are all part of the TmdbSpider class!!!\n\nAnyways…\nWhat just happened in the last line? Well, in the “yield scrapy.Request” line, we first tell scrapy to fetch this url. After it fetches the “full cast and crew” url, we obtain the response object. This response object is automatically passed in as the second argument to the callback function, parse_full_credits. And, remember from PIC16A that “self” is always passed in as the first argument. This is why parse_full_credits function has no arguments specified.\nparse_full_credits: 1. First argument: self (automatic) 2. Second argument: response object from cast_url (automatic)\nWhat does the parse_full_credits() method do? Well, let’s define it now!"
  },
  {
    "objectID": "posts/hw-2/index.html#part-2a-parse-full-credits-method",
    "href": "posts/hw-2/index.html#part-2a-parse-full-credits-method",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 2A: Parse full credits Method",
    "text": "Part 2A: Parse full credits Method\nThe second class method we will define is parse_full_credits(). This function takes in two arguments: a reference to the current instance of the class “self”, and the response object generated from the previous parse method. It assumes we start on the “full cast and crew” wegpage for a specific movie. Here is how it works:\n\nLocate the “cast” table of the webpage.\n\n\nTo do this, we will search for an h3 element containing “Cast” using Xpath.\nWe must ensure we are only in the cast table, or else we might get non-actors in our list (aka, members from the production crew).\n\n\nObtain the list of cast members.\n\n\nSelect the first “ordered list” (ol) element that appears after the h3 element. We can do this using “following-sibling::ol[1]”.\nNote that the ordered list element is not a “child” of the h3 element, which is why we use the “following-sibling” expression.\n\n\nObtain the actor urls.\n\n\nUsing the xpath object of cast members, navigate into the list element that contains a div element with a class attribute containing “info”.\nThen, look for a &lt;&gt; tag after the div class. This &lt;&gt; tag may or may not be the direct descendent of the div element, which is why we use the “descendant-or-self” line.\nFinally, navigate into the &lt;&gt; tag and grab the url inside it. Use the “getall()” method to obtain all such links.\n\n\nYield scrapy requests for each actor url, using the parse_actor_page method as a callback function.\n\nHere is what the function should look like.\n\ndef parse_full_credits(self, response): \n        \"\"\"\n        Assumptions: \n            Assume we are starting on a \"full cast and crew\" page for one movie. \n        Effects: \n            Navigate to each actor's page for the specific movie (note - ACTOR - not any other crew member!) \n            Data outputs: A yielded scrapy request calling parse_actor_page() method for every single actor's link in this webpage. \n        \"\"\"\n        \n        # Step 1: Locate cast table of webpage (h3 tag contains \"Cast\")  \n        # Step 2: Create an xpath object from this \"Cast\" table, called table1 \n        table1 = response.xpath('//h3[contains(., \"Cast\")]/following-sibling::ol[1]')\n        \n        # Step 3: Obtain all the actor urls by finding the proper link for each actor  \n        urls_actors = table1.xpath('.//li//div[contains(@class, \"info\")]/descendant-or-self::*/p/a/@href').getall()\n        # notice that we use getall() method to obtain ALL actor urls \n        \n        # Step 4: Yield scrapy requests for each actor url \n        for link in urls_actors: \n            yield scrapy.Request(url = response.urljoin(link), callback = self.parse_actor_page) \n            # we use urljoin as opposed to simple string concatonation to ensure that relative urls are formed properly \n\nHere is what the HTML element looks like. In this HTML element, we are trying to obtain the second “a href=”/person/368-reese-witherspoon” text to get the link for the actor page. We specifically try to get the second one, and not the first one, because each actor has two links and we do not want more than one link per actor.\n\n\n\nScreenshot 2024-02-05 at 1.31.27 PM.png\n\n\nIn our function, just like before, we do not put any arguments into the parse_actor_page() function because the second argument is automatically the response object fetched by the url we specified, and the first argument is automatically “self”. But what is the parse_actor_page() function? Let’s define it right now!"
  },
  {
    "objectID": "posts/hw-2/index.html#part-3a-parse-actor-page",
    "href": "posts/hw-2/index.html#part-3a-parse-actor-page",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 3A: Parse actor page",
    "text": "Part 3A: Parse actor page\nThe final class method we will define is the parse_actor_page() method. This function takes in two arguments, (1) a reference to the current instance of the class “self”, and (2) the response object generated from the previous class method we defined. It assumes we start on an actor’s webpage. It will yield a dictionary object for each actor and each movie they played in. Here is how it works:\n\nGet the actor’s name: It will be on the very top of the webpage in the &lt;&gt; title element.\nLocate the “acting” table in the actor’s webpage and create an xpath object from it called “table”.\n\n\nWe do this by first finding the h3 element with text element “Acting”, and then we select the element that occurs directly after it.\nThis is why we use the “following-sibling” code.\n\n\nObtain each movie / tv show name from the acting “table”.\n\n\nThis is done by navigating to the &lt;&gt; element within the acting “table”.\n\n\nYield a key-pair value containing {actor_name, movie_or_TV_name} for every movie / tv show that the actor played in.\n\n\ndef parse_actor_page(self, response): \n        \"\"\"\n        Assumptions: \n            Assume we are starting on an actor's webpage. \n        Effects: \n            Does not navigate to any new webpages.  \n            Data outputs: A yielded dictionary, each key-value pair containing the movie that an actor acted in along with the actor's name. \n        \"\"\"\n        \n        # Step 1: obtain the actor's name from the Title of the webpage \n        actor_name = response.css('h2.title &gt; a::text').get()\n        \n        # Step 2: Locate the acting table in the webpage, and select the element that occurs immediately after it \n        table = response.xpath('//h3[text()=\"Acting\"]/following-sibling::table[1]')\n        \n        # Step 3: Obtain the relevant movie / tv show names from each acting \"table\"  \n        movie_names = table.css('td.role.true.account_adult_false.item_adult_false &gt; a.tooltip &gt; bdi::text').getall()\n        # notice that we use \"getall()\" method to obtain ALL movie / tv show names  \n        \n        # Yield a key-value pair for each movie / tv show in this actor's webpage \n        for movie_or_TV_name in movie_names: \n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name} \n\nHere is what the HTML element looks like. In this HTML element, we are trying to obtain the text within the &lt;&gt; element because that gives us the movie / tv show name.\n\n\n\nScreenshot 2024-02-05 at 5.19.29 PM.png"
  },
  {
    "objectID": "posts/hw-2/index.html#part-0b-setting-up-scraper",
    "href": "posts/hw-2/index.html#part-0b-setting-up-scraper",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 0B: Setting up scraper",
    "text": "Part 0B: Setting up scraper\nNow that we know how to build our TMDB Spider, let’s actually set it up and use it to find recommendations based on Legally Blonde. The first step is to set up your local machine for webscraping. Here are the first few steps:\nIn your terminal, 1. Activate the Python environment of your choice 2. Navigate into the directory where you want your scraper files to be\n3. Run the following lines in your terminal:\nscrapy startproject TMDB_scraper cd TMDB_scraper\nThis will create a lot of folders and files. Do not worry about these. Here are the next steps: 1. Inside your spider directory, add a file called “tmdb_spider.py”. Inside this file, write out the tmdbSpider class we just defined. 2. In your settings.py file, add this line: “USER_AGENT = ‘Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0’”. This will ensure we do not run into any 403 errors."
  },
  {
    "objectID": "posts/hw-2/index.html#part-1b-running-the-scraper",
    "href": "posts/hw-2/index.html#part-1b-running-the-scraper",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 1B: Running the scraper",
    "text": "Part 1B: Running the scraper\nNow, we will run our scraper for Legally Blonde. The TMDB link for Legally Blonde is:\nhttps://www.themoviedb.org/movie/8835-legally-blonde\nSo, our “subdir” argument should be 8835-legally-blonde. Navigate into the directory where you want your results file to be, and run the following line in your terminal:\nscrapy crawl tmdb_spider -o results.csv -a subdir=8835-legally-blonde\nThis line will webscrape the Legally Blonde database using the tmdb spider class we just defined. The “subdir” argument is passed into the very first “parse” method, and the webscraper navigates to the appropriate webpages from there."
  },
  {
    "objectID": "posts/hw-2/index.html#part-2b-accessing-and-analyzing-results.csv-file",
    "href": "posts/hw-2/index.html#part-2b-accessing-and-analyzing-results.csv-file",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 2B: Accessing and analyzing results.csv file",
    "text": "Part 2B: Accessing and analyzing results.csv file\nAfter running this line in your terminal, wait till you see a message that the Spider has closed in your terminal. Then, in the folder that you are currently in, there should be a file called “results.csv”. This should contain two columns: 1. Actor name 2. Movie name\nNow, let’s analyze these results to build a recommender system.\nFirst, let’s import this results.csv file and turn it into a pandas dataframe for easy analysis.\n\n# import pandas \nimport pandas as pd\n\n# import results csv file as dataframe \nresults = pd.read_csv(\"results.csv\") \n\n# preview results dataframe\nresults.head()\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nReese Witherspoon\nTiny Trailblazers\n\n\n1\nReese Witherspoon\nTracy Flick Can't Win\n\n\n2\nReese Witherspoon\nGreat Performers: 9 Kisses\n\n\n3\nReese Witherspoon\nYou’re Cordially Invited\n\n\n4\nReese Witherspoon\nLegally Blonde 3\n\n\n\n\n\n\n\nGreat! Let’s see how many actors were in the original “Legally Blonde” movie, and how many total actors and movie / TV show names there are.\n\nactors_in_legally_blonde = results[results['movie_or_TV_name'] == 'Legally Blonde']['actor'].unique()\nmovies_total = results['movie_or_TV_name'].unique()\n\nprint(\"All the actors in Legally Blonde: \\n\", actors_in_legally_blonde, \"\\n\")\n\nprint(\"All the movies that are in the results file: \\n\", movies_total, \"\\n\")\nprint(\"Number of movies / TV shows in the results file: \\n\", results['movie_or_TV_name'].nunique())\n\nAll the actors in Legally Blonde: \n ['Reese Witherspoon' 'Lily' 'Kelly Driscoll' 'Sasha Barrese' 'Moonie'\n 'Kennedy Stone' 'Elizabeth Matthews' 'Richard Hillman' 'John Kapelos'\n 'Patricia Kimes' 'Jodi Harris' 'Nectar Rose' 'Terence Michael'\n 'John Cantwell' 'Ondrea de Vincentis' 'Chaney Kley' 'Melissa Anne Young'\n 'Brody Hutzler' 'Lacey Beeman' 'Jason Christopher' 'Lisa K. Wyatt'\n 'Corinne Reilly' 'Doug Spinuzza' 'Niklaus Lange' 'Victoria Mahoney'\n 'Tane McClure' 'David Moreland' 'Allyce Beasley' 'Kevin Cooney'\n 'Cici Lau' 'Natalie Barish' 'Lisa Arch' 'Francesca P. Roberts'\n 'Kimberly McCullough' \"Shannon O'Hurley\" 'Ted Rooney' 'Kelly Nyks'\n 'Samantha Lemole' 'Michael B. Silver' 'Ted Kairys' 'Bruce Thomas'\n 'Meredith Scott Lynn' 'Linda Cardellini' 'Raquel Welch' 'Greg Serano'\n 'Osgood Perkins' 'Wayne Federman' 'Jessica Cauffiel' 'Alanna Ubach'\n 'Victor Garber' 'James Read' 'Jennifer Coolidge' 'Holland Taylor'\n 'Matthew Davis' 'Selma Blair' 'Luke Wilson' 'Ali Larter'] \n\nAll the movies that are in the results file: \n ['Tiny Trailblazers' \"Tracy Flick Can't Win\" 'Great Performers: 9 Kisses'\n ... 'Giving It Up' 'Varsity Blues' 'Just Shoot Me!'] \n\nNumber of movies / TV shows in the results file: \n 1775"
  },
  {
    "objectID": "posts/hw-2/index.html#part-3b-building-our-recommender-system",
    "href": "posts/hw-2/index.html#part-3b-building-our-recommender-system",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 3B: Building our recommender system",
    "text": "Part 3B: Building our recommender system\nThere are 1775 movies/TV shows to choose from! How do we know which one we will like best? One way to do this is to see which movies / TV shows have the highest number of shared actors with Legally Blonde. Then, we can create a visualization with the top 10 movies / TV shows and the number of shared actors they contain.\nLet’s make this visualization pink, Elle Woods’ favorite color!!\n\n\n\nImage 2-5-24 at 6.40 PM.jpg\n\n\nImage source\n\n# import matplotlib \nimport matplotlib.pyplot as plt \n\n# create a new dataframe for shared actors \nsame_actors = results[results['actor'].isin(actors_in_legally_blonde) & (results['movie_or_TV_name'] != 'Legally Blonde')] \n\n# find out how many shared actors per movie \nsame_actors_value = same_actors['movie_or_TV_name'].value_counts()\n\n# create a new dataframe with top 10 movies / TV shows \ntop_movie_TV_show = same_actors_value.nlargest(10)\n\n# plot bar chart accordingly \nplt.figure(figsize=(10, 6))\n# make sure the color is pink! \ntop_movie_TV_show.plot(kind='bar', color='pink')\nplt.title('Most recommended movies / TV shows for you based on Legally Blonde')\nplt.xlabel('Movie / TV Show Name')\nplt.ylabel('Number of Shared Actors')\nplt.show()"
  },
  {
    "objectID": "posts/hw-2/index.html#part-4b-summary-and-takeaways",
    "href": "posts/hw-2/index.html#part-4b-summary-and-takeaways",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 4B: Summary and takeaways",
    "text": "Part 4B: Summary and takeaways\nWow! Now we have a visualization for a recommender system based on Legally Blonde. I am shocked there are so many true crime shows on there! I guess Reese Witherspoon really stuck to the “Legal” movie genre. Good for her! She’s such a slay girlboss queen.\nSomething else that is pretty cool about this visualization is that it not only shows you which movies and TV shows are recommended for you, but at which level they are recommended for you based on the number of actors.\nNote that you can do this for any movie of your choice using the scraper we built!\n\nThanks for reading!! And remember: Whoever said orange is the new pink was seriously disturbed."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This python code is known as one of the most complex pieces of code ever written. If you run this in your IDE, it might blow your mind:\n\nprint(\"hello world\") \n\nhello world"
  },
  {
    "objectID": "posts/hw-3/index.html",
    "href": "posts/hw-3/index.html",
    "title": "How to create a simple (pink) Message Bank Webapp that submits and views messages using Python and Flask",
    "section": "",
    "text": "Today we will be creating a webapp using Python and Flask to submit and view messages. A “webapp” is exactly what it sounds like: it’s like an application (or, app) you can deploy it on the internet (or, web). In today’s blog, the app won’t actually be published on the internet, but we will still be accessing it on our local browser.\nI will be describing how to set up this project in detail. If you want more details, a link to the project repository is here:\nhttps://github.com/anikamisra/pic16b-hw3-webapp/"
  },
  {
    "objectID": "posts/hw-3/index.html#function-1-get_message_db",
    "href": "posts/hw-3/index.html#function-1-get_message_db",
    "title": "How to create a simple (pink) Message Bank Webapp that submits and views messages using Python and Flask",
    "section": "Function 1: get_message_db",
    "text": "Function 1: get_message_db\nThis function handles database creation for messages. First, make sure to import the necessary packages into app.py:\n\nfrom flask import Flask, request, render_template\nimport sqlite3\nfrom flask import g\nimport random\n\nNow, let’s create the method. The way this function works is: 1. First it will check if the SQL message database, called message_db, exists. 2. If the database doesn’t exist yet, then it will create a new one. 3. Open a connection to the message database and store it in g.message_db 4. Create a cursor with the database object (used for SQL commands) 5. Execute SQL command to create a “messages” table if it does not already exist 6. Return the database connection.\nHere’s a look at the code:\n\ndef get_message_db():\n    \"\"\"\n    handles database creation for messages  \n    \"\"\"\n    # check if message_db exists\n    if 'message_db' not in g:\n        # connect database \n        g.message_db = sqlite3.connect('messages_db.sqlite')\n    cursor = g.message_db.cursor()\n\n    # check for messages table  \n    cursor.execute('CREATE TABLE IF NOT EXISTS messages (id INTEGER PRIMARY KEY, handle TEXT, message TEXT)')\n    return g.message_db\n\nGreat! We’ve created our first method. But, what will this functio be used with? Now, let’s define the second method in our aplication script file:"
  },
  {
    "objectID": "posts/hw-3/index.html#function-2-insert_message",
    "href": "posts/hw-3/index.html#function-2-insert_message",
    "title": "How to create a simple (pink) Message Bank Webapp that submits and views messages using Python and Flask",
    "section": "Function 2: insert_message",
    "text": "Function 2: insert_message\nThis function will handle user insertion of messages into the database.\nHere is how it works: 1. Connect database by calling get_message_db() 2. Connect cursor 3. Execute SQL command to insert message and user id into messages table 4. Run commit() function to ensure the row insertion has been saved 5. Close database connection\nHere is what the code looks like:\n\ndef insert_message(request):\n    \"\"\"\n    insert message into database after user submission \n    \"\"\"\n    # connect database \n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute('INSERT INTO messages (handle, message) VALUES (?, ?)', (request.form['user'], request.form['message']))\n    # save \n    db.commit()\n    # close the database connection \n    db.close()\n\nSo, this function works by calling the previously defined function, get_message_db(). Furthermore, we can see how it specifically inserts the username and the message of the person who posted it. Don’t worry too much about “resquest.form” for now - we will get into this later."
  },
  {
    "objectID": "posts/hw-3/index.html#function-3-random_messages",
    "href": "posts/hw-3/index.html#function-3-random_messages",
    "title": "How to create a simple (pink) Message Bank Webapp that submits and views messages using Python and Flask",
    "section": "Function 3: random_messages",
    "text": "Function 3: random_messages\nThe last non-route function we will define in our application instance file is the random_messages function. This function is used to generate 5 random messages that our in our database (or, fewer, if there aren’t 5 submitted messages yet). We will call this function when we view the messages in our webapp. Here is how it works: 1. Connect to database by calling Function # 1 2. Connect cursor and execute SQL command of selecting all messages 3. Define a cursor object, messages, that selects all messages and usernames submitted 4. Close database connection 5. Utilize random library to generate n messages from “messages” cursor object. If there are less than n messages in the database, then select however many are in the database.\nLet’s take a look at the code:\n\ndef random_messages(n):\n    \"\"\"\n    returns n (or fewer) random messages for viewing \n    \"\"\"\n    # connect database and get messages\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute('SELECT * FROM messages')\n    messages = cursor.fetchall()\n    # close connection \n    db.close()\n    # return n messages (or, fewer if there aren't n messages)\n    return random.sample(messages, min(n, len(messages)))\n\nGreat! Now, we have defined our non-route functions in the application, let’s get onto the route functions."
  },
  {
    "objectID": "posts/hw-3/index.html#function-4-home-page",
    "href": "posts/hw-3/index.html#function-4-home-page",
    "title": "How to create a simple (pink) Message Bank Webapp that submits and views messages using Python and Flask",
    "section": "Function 4: home page",
    "text": "Function 4: home page\nThis function will render the HTML template to create our home page. We want our default page to be a simple page that allows the user to navigate to wherever they want to go. So, we will only use the ‘GET’ method here.\n\n# url route function for home page     \n@app.route('/', methods=['GET'])\ndef home():\n    # render submit.html page \n    return render_template('base.html')\n\nWe specify the method as ‘GET’ because we will not be posting any data here. It is simply the home page that takes us to either submitting, or viewing a message. This template is the “base” that the other HTML pages, which are view.html and submit.html, will “inherit” from. Here is what base.html looks like:\n\nbase.html\n\n\n\nScreenshot 2024-02-14 at 6.54.33 PM.png\n\n\nAs we can see, there are only two links on the page that take us to “submit” and “messages”. Now, let’s define the route functions for these two pages. Furthermore, we can incorporate the style.css file that we want in order to customize our website. In my style.css file, I made the background of my webpage pink. Here is what the home page looks like:\n\n\n\nScreenshot 2024-02-14 at 6.13.03 PM.png\n\n\n(We will learn more about how to get this running on your local browser in Section 3.)\nSo pink, so beautiful! (Note, I made it pink and changed the font in “style.css”.)\nNow, let’s create the route functions to render “submit.html” and “view.html”, which will allow us to actually use those cute links that are on our home page."
  },
  {
    "objectID": "posts/hw-3/index.html#function-5-submit-page",
    "href": "posts/hw-3/index.html#function-5-submit-page",
    "title": "How to create a simple (pink) Message Bank Webapp that submits and views messages using Python and Flask",
    "section": "Function 5: Submit page",
    "text": "Function 5: Submit page\nIn this function, we will render the submit.html template, which will allow us to actually submit messages! Yay! In order to do this, we will use both the ‘GET’ and ‘POST’ methods, since we need to respond to HTTP requests and also transmit the data of the submitted message. Here is how it works:\n\nSet message and user to none before anything has been inputted.\nOnce the user has submitted the form (“post” button has been clicked), send in the data as an HTTP request.\n\n\nTo do this, we use Flask’s “request.form”, which is a type of special dictionary used to store these HTTP requests\n\n\nCall the previously defined insert_message function with the request as an argument.\n\n\nRemember how we used “(request.form[‘user’], request.form[‘message’]) in the SQL command? This is where it comes from.\n\n\nRender submit.html template. We will “pass in” the user’s name and message because it will be used in the html page.\n\nHere is what the function should look like:\n\n# url route function for submit page  \n@app.route('/submit', methods=['GET', 'POST'])\ndef submit():\n    message = None\n    user = None\n    # handle user posting \n    if request.method == 'POST':\n        user = request.form['user']\n        message = request.form['message']\n        # call insert_message function \n        insert_message(request)\n    return render_template('submit.html', user=user, message=message)\n\nWe will pass in user into submit.html because we want to thank the user for their submission. Here is what the submit.html page should look like:\n\nsubmit.html\n\n\n\nScreenshot 2024-02-14 at 6.56.20 PM.png\n\n\nAs we can see, submit.html extends the base.html page, and thanks the user for their submission. It also stores a box for message and user. We will see what this looks like in Section 3. As of right now, the “message” variable is not used, but it is still handy to pass it in in case we want to customize our submit.html function later and use that variable."
  },
  {
    "objectID": "posts/hw-3/index.html#function-6-view-messages-page",
    "href": "posts/hw-3/index.html#function-6-view-messages-page",
    "title": "How to create a simple (pink) Message Bank Webapp that submits and views messages using Python and Flask",
    "section": "Function 6: View messages page",
    "text": "Function 6: View messages page\nWe are almost done with our functions! Finally, let’s create a function to render the view.html page. This will allow us to see 5 (or less) randomly generated messages that have been inputted.\nOnce again, we are only using the ‘GET’ method here because we are not transmitting any data - only receiving the HTTP requests. The “messages” variable utilizes the previously defined random_messages() function to obtain and store 5 random messages that are in our messages table in the database. Then, it renders the view.html page and passes in the 5 obtained messages.\nHere is what the function looks like:\n\n#  url route function for view messages page \n@app.route('/messages', methods=['GET'])\ndef messages(): \n    # generate 5 random messages to view \n    messages = random_messages(5)\n    return render_template('view.html', messages=messages)\n\nBut how do the messages get used? Let’s take a look at view.html:\n\nview.html\n\n\n\nScreenshot 2024-02-14 at 6.56.48 PM.png\n\n\nAs we can see, we display the 5 random messages we obtained in an ordered list.\nEvery time we click “View messages” on our webapp, it will rerun the url route function and obtain 5 NEW randomly selected messages.\nNow, it’s time to see all this in action! In the next section, I will use screenshots to show how the Message Bank webapp really works!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in my Quarto blog. Just testing things out. Here is a painting I made two days ago."
  },
  {
    "objectID": "posts/hw-6/index.html",
    "href": "posts/hw-6/index.html",
    "title": "Experimenting with different Keras Models to classify fake news",
    "section": "",
    "text": "In today’s blog post, we will be classifying fake news articles. We will be experimenting with different types of keras layers. We will also analyze our best model’s word embeddings and see what this means in context of politics.\nNote: when we want to determine whether a news article is legitimate or trustworthy, we can look at 2 components: 1. First, obviously, the article itself. It may contain certain key words and hyperboles that signify misinformation. 2. Another great way to tell if news is take is by looking at the title, which often overexxagerates to serve as “clickbait”.\nIn today’s post, we will be building a classifier that, given an article, can determine if it is fake news or not."
  },
  {
    "objectID": "posts/hw-6/index.html#welcome",
    "href": "posts/hw-6/index.html#welcome",
    "title": "Experimenting with different Keras Models to classify fake news",
    "section": "",
    "text": "In today’s blog post, we will be classifying fake news articles. We will be experimenting with different types of keras layers. We will also analyze our best model’s word embeddings and see what this means in context of politics.\nNote: when we want to determine whether a news article is legitimate or trustworthy, we can look at 2 components: 1. First, obviously, the article itself. It may contain certain key words and hyperboles that signify misinformation. 2. Another great way to tell if news is take is by looking at the title, which often overexxagerates to serve as “clickbait”.\nIn today’s post, we will be building a classifier that, given an article, can determine if it is fake news or not."
  },
  {
    "objectID": "posts/hw-6/index.html#layers",
    "href": "posts/hw-6/index.html#layers",
    "title": "Experimenting with different Keras Models to classify fake news",
    "section": "Layers",
    "text": "Layers\nIn general, keras layers help transform our inputs to our outputs in the model. Each layer performs an operation in our model that gets passed as an output to the next layer. For our model, let’s use the following text vectorization layer:\n\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\n# standardize text data by making everything lower case and removing punctuation\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\n# transform text data into a layer to be used in the model\ntext_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\n# adapt vectorization layer to text\n# map this text vectorization layer from x[0] (title) AND x[1] (text)\ntext_vectorize_layer.adapt(train_data.map(lambda x, y: tf.concat([x[0], x[1]], axis=0)))\n\nLet’s also create an embedding layer for the models.\n\nembed_dim = 128\nembedding_layer = Embedding(size_vocabulary, embed_dim, name=\"embedding\")\n\nWhat just happened? With NLP, we usually need these two layers:\n\nTextVectorization\n\nThis layer preprocesses the words and turns words into tokens for machine learning.\n\nEmbedding layer\n\nThe embedding layer assigns a representation of each word in terms of semantic meaning, which allows us to see which words are most closely related to each other. A higher embedding dimension means that the model is able to find relationships between words’ meanings easier. However, this comes with the tradeoff of overfitting.\nIn our model, an embedding dimension of 128 seems to have good results, as we will see.\nWhy use the same layers for all models? We are trying to answer the question: “When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?” By keeping all control variables the same, we can focus on solely the inputs of the different models and then truly compare."
  },
  {
    "objectID": "posts/hw-6/index.html#model-1-article-title",
    "href": "posts/hw-6/index.html#model-1-article-title",
    "title": "Experimenting with different Keras Models to classify fake news",
    "section": "Model 1: Article title",
    "text": "Model 1: Article title\nFor the first model, we will use only the article title as the input when creating our model to predict whether the news article is fake or not. Luckily, we do not need to create a new tf.data.Dataset object from the train object, because we can just specify the appropriate input to the keras.Model.\nWe will be using Functional API rather than Sequential API for these models because we will eventually need to use the same layer in multiple parts of the model, in model #3.\n\n# create an input layer for the model\ntitle_input = layers.Input(shape=(1,), dtype=tf.string, name='title')\n\n# now define the features of the model using the input\n# first, pass through the text vectorization and embedding layers\ntitle_features = text_vectorize_layer(title_input)\ntitle_features = embedding_layer(title_features)\n\n# next, we want to prevent over fitting:\n\n# reduces dimensionality and simplifies model\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\n# and use dropout layers to avoid overfitting\ntitle_features = Dropout(0.2)(title_features)\n\n# finally, create prediction layer\n# use 1 neuron because this is classification problem\ntitle_pred = Dense(1, activation='sigmoid')(title_features)\n\n# input these layers into keras Model\nmodel1 = Model(title_input, title_pred)\n\nLet’s take a look at what this model looks like.\n\nutils.plot_model(model1, \"model1.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nThis model takes in an input layer of shape 1, transforms it to the correct shape for the other layers that we defined, and then finally outputs a prediction layer of shape 1.\nNow, let’s compile and then fit the model.\nCompiling: This is when we specify the loss function, optimizer, and metric we will use before training. We will use binary crossentropy because we have a binary classification model.\nFitting: Finally, we will use our predefined layers to train and the model. 10 epochs appears to give us good results without overfitting.\n\nmodel1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# use x[0] because we are looking at titles only\nhistory1 = model1.fit(train_data.map(lambda x, y: (x[0], y)),\n                      epochs=10,\n                      validation_data=val_data.map(lambda x, y: (x[0], y)))\n\nWARNING:tensorflow:AutoGraph could not transform &lt;function &lt;lambda&gt; at 0x7b133a8f7490&gt; and will run it as-is.\nCause: could not parse the source code of &lt;function &lt;lambda&gt; at 0x7b133a8f7490&gt;: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda x, y: (x[0], y)\n\nMatch 1:\nlambda x, y: (x[0], y)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING:tensorflow:AutoGraph could not transform &lt;function &lt;lambda&gt; at 0x7b133a8a5480&gt; and will run it as-is.\nCause: could not parse the source code of &lt;function &lt;lambda&gt; at 0x7b133a8a5480&gt;: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda x, y: (x[0], y)\n\nMatch 1:\nlambda x, y: (x[0], y)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n\n\nWARNING: AutoGraph could not transform &lt;function &lt;lambda&gt; at 0x7b133a8f7490&gt; and will run it as-is.\nCause: could not parse the source code of &lt;function &lt;lambda&gt; at 0x7b133a8f7490&gt;: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda x, y: (x[0], y)\n\nMatch 1:\nlambda x, y: (x[0], y)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform &lt;function &lt;lambda&gt; at 0x7b133a8a5480&gt; and will run it as-is.\nCause: could not parse the source code of &lt;function &lt;lambda&gt; at 0x7b133a8a5480&gt;: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda x, y: (x[0], y)\n\nMatch 1:\nlambda x, y: (x[0], y)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nEpoch 1/10\n180/180 [==============================] - 8s 38ms/step - loss: 0.6898 - accuracy: 0.5279 - val_loss: 0.6871 - val_accuracy: 0.5173\nEpoch 2/10\n180/180 [==============================] - 6s 31ms/step - loss: 0.6779 - accuracy: 0.5803 - val_loss: 0.6672 - val_accuracy: 0.5424\nEpoch 3/10\n180/180 [==============================] - 6s 32ms/step - loss: 0.6478 - accuracy: 0.7175 - val_loss: 0.6252 - val_accuracy: 0.8207\nEpoch 4/10\n180/180 [==============================] - 6s 33ms/step - loss: 0.5977 - accuracy: 0.8080 - val_loss: 0.5684 - val_accuracy: 0.8482\nEpoch 5/10\n180/180 [==============================] - 6s 31ms/step - loss: 0.5379 - accuracy: 0.8389 - val_loss: 0.5087 - val_accuracy: 0.8531\nEpoch 6/10\n180/180 [==============================] - 6s 35ms/step - loss: 0.4817 - accuracy: 0.8482 - val_loss: 0.4568 - val_accuracy: 0.8547\nEpoch 7/10\n180/180 [==============================] - 6s 32ms/step - loss: 0.4335 - accuracy: 0.8575 - val_loss: 0.4154 - val_accuracy: 0.8609\nEpoch 8/10\n180/180 [==============================] - 9s 49ms/step - loss: 0.3963 - accuracy: 0.8639 - val_loss: 0.3829 - val_accuracy: 0.8656\nEpoch 9/10\n180/180 [==============================] - 6s 33ms/step - loss: 0.3665 - accuracy: 0.8702 - val_loss: 0.3576 - val_accuracy: 0.8709\nEpoch 10/10\n180/180 [==============================] - 5s 28ms/step - loss: 0.3424 - accuracy: 0.8736 - val_loss: 0.3367 - val_accuracy: 0.8758\n\n\nLet’s also quickly write a function to plot the history of the training accuracy and validation accuracy throughout time.\n\nfrom matplotlib import pyplot as plt\n\ndef plot_history(history, model_name = \"Model\"):\n  # we want both training and validation accuracies\n  plt.plot(history.history[\"accuracy\"], label = \"training\")\n  plt.plot(history.history[\"val_accuracy\"], label = \"validation\")\n  # x tells us \"time\", or number of epochs\n  plt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\n  plt.legend()\n  plt.title(model_name)\n  plt.show()\n\n\nplot_history(history1, \"Model 1\")\n\n\n\n\n\n\n\n\nAs we can see, both the training data and the validation data appear to be increasing and plateauing, which is a good sign. There is no sign of overfitting (because the validation accuracy is not decreasing). Let’s take a look at the accuracy of this model:\n\n# obtain values of title, text, and output separately from the validation data\ntitle_val = [title.numpy() for (title, text), output in val_data]\ntext_val = [text.numpy() for (title, text), output in val_data]\noutput_val = [output.numpy() for (title, text), output in val_data]\n\n# concatenate to use in evaluate function\ntitle_val = np.concatenate(title_val)\noutput_val = np.concatenate(output_val)\n\n# evaluate\nloss1, accuracy1 = model1.evaluate(title_val, output_val, verbose=2)\nprint(f'Model 1 Accuracy: {accuracy1}')\n\n141/141 - 1s - loss: 0.3367 - accuracy: 0.8758 - 971ms/epoch - 7ms/step\nModel 1 Accuracy: 0.8757777810096741\n\n\nThe accuracy of the first model is 87.5% on the validation data set. The fact that this is based on the titles alone is impressive! That means that the titles of articles can tell you a lot about whether or not they are fake news.\nBut intuitively, it seems like the actual text in the article would be a better indicator. There is simply more data and more information there. This brings us to model 2."
  },
  {
    "objectID": "posts/hw-6/index.html#model-2-article-text",
    "href": "posts/hw-6/index.html#model-2-article-text",
    "title": "Experimenting with different Keras Models to classify fake news",
    "section": "Model 2: Article text",
    "text": "Model 2: Article text\nFor this second model, we will construct it similarly to how we constructed model 1, except we will only be looking at article text as an input.\nOur embedding layer and text vectorization layers will remain the same. So we can just get started by defining the input layers for the models.\nIn fact, because we are sharing the textvectorization and embedding layers, and we want to have the same overfitting reduction techniques as model 1, we can just reuse the layers from model 1 but then train on the “text” instead of titles.\n\n# create an input layer for the model\n# everything remains the same except for input! here we use text.\ntext_input = layers.Input(shape=(1,), dtype=tf.string, name='text')\n\n# now define the features of the model using the input\n# first, pass through the text vectorization and embedding layers\n# note we use the same text_vectorize layer for text and title!\ntext_features = text_vectorize_layer(text_input)\ntext_features = embedding_layer(text_features)\n\n# next, we want to prevent over fitting:\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = Dropout(0.2)(text_features)\n\n# finally, create prediction layer\ntext_pred = Dense(1, activation='sigmoid')(text_features)\n\n# input these layers into keras Model\nmodel2 = Model(text_input, text_pred)\n\nLet’s visualize the model:\n\nmodel2 = Model(text_input, text_pred)\n# visualize model\nutils.plot_model(model2, \"model2.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nThis model has the same shapes and layers as model 1, except the input has text instead of title.\nNow, just as before, we will compile the model and then train it on the text data.\n\nmodel2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory2 = model2.fit(train_data.map(lambda x, y: (x[0], y)),\n                      epochs=18,\n                      validation_data=val_data.map(lambda x, y: (x[1], y)))\n\nEpoch 1/18\n180/180 [==============================] - 9s 34ms/step - loss: 0.1686 - accuracy: 0.9309 - val_loss: 13.7182 - val_accuracy: 0.6160\nEpoch 2/18\n180/180 [==============================] - 6s 34ms/step - loss: 0.1659 - accuracy: 0.9317 - val_loss: 13.8068 - val_accuracy: 0.6173\nEpoch 3/18\n180/180 [==============================] - 6s 35ms/step - loss: 0.1640 - accuracy: 0.9323 - val_loss: 13.8580 - val_accuracy: 0.6193\nEpoch 4/18\n180/180 [==============================] - 7s 37ms/step - loss: 0.1626 - accuracy: 0.9344 - val_loss: 13.8726 - val_accuracy: 0.6202\nEpoch 5/18\n180/180 [==============================] - 5s 29ms/step - loss: 0.1615 - accuracy: 0.9346 - val_loss: 13.8720 - val_accuracy: 0.6216\nEpoch 6/18\n180/180 [==============================] - 7s 37ms/step - loss: 0.1598 - accuracy: 0.9351 - val_loss: 13.8805 - val_accuracy: 0.6242\nEpoch 7/18\n180/180 [==============================] - 6s 33ms/step - loss: 0.1584 - accuracy: 0.9362 - val_loss: 13.8879 - val_accuracy: 0.6251\nEpoch 8/18\n180/180 [==============================] - 6s 31ms/step - loss: 0.1575 - accuracy: 0.9357 - val_loss: 13.9043 - val_accuracy: 0.6258\nEpoch 9/18\n180/180 [==============================] - 6s 32ms/step - loss: 0.1562 - accuracy: 0.9366 - val_loss: 13.9245 - val_accuracy: 0.6273\nEpoch 10/18\n180/180 [==============================] - 6s 35ms/step - loss: 0.1543 - accuracy: 0.9370 - val_loss: 13.9830 - val_accuracy: 0.6284\nEpoch 11/18\n180/180 [==============================] - 6s 33ms/step - loss: 0.1535 - accuracy: 0.9383 - val_loss: 14.0372 - val_accuracy: 0.6284\nEpoch 12/18\n180/180 [==============================] - 6s 32ms/step - loss: 0.1530 - accuracy: 0.9379 - val_loss: 14.0990 - val_accuracy: 0.6291\nEpoch 13/18\n180/180 [==============================] - 6s 35ms/step - loss: 0.1524 - accuracy: 0.9380 - val_loss: 14.1503 - val_accuracy: 0.6296\nEpoch 14/18\n180/180 [==============================] - 6s 32ms/step - loss: 0.1509 - accuracy: 0.9393 - val_loss: 14.2355 - val_accuracy: 0.6302\nEpoch 15/18\n180/180 [==============================] - 6s 34ms/step - loss: 0.1501 - accuracy: 0.9399 - val_loss: 14.3156 - val_accuracy: 0.6304\nEpoch 16/18\n180/180 [==============================] - 6s 36ms/step - loss: 0.1489 - accuracy: 0.9396 - val_loss: 14.4103 - val_accuracy: 0.6300\nEpoch 17/18\n180/180 [==============================] - 7s 36ms/step - loss: 0.1486 - accuracy: 0.9403 - val_loss: 14.5005 - val_accuracy: 0.6284\nEpoch 18/18\n180/180 [==============================] - 6s 33ms/step - loss: 0.1475 - accuracy: 0.9412 - val_loss: 14.5977 - val_accuracy: 0.6287\n\n\nAnd, let’s see what the training history of model 2 looks like.\n\nplot_history(history2, \"Model 2\")\n\n\n\n\n\n\n\n\nHm… this doesn’t look the most accurate. Let’s look at the accuracy of model 2 on the validation dataset:\n\n# concatenate text to use in evaluate function\ntext_val = np.concatenate(text_val)\n\n# evaluate (output_val was already created in model 1)\nloss2, accuracy2 = model2.evaluate(text_val, output_val, verbose=2)\nprint(f'Model 1 Accuracy: {accuracy2}')\n\n141/141 - 1s - loss: 14.5977 - accuracy: 0.6287 - 941ms/epoch - 7ms/step\nModel 1 Accuracy: 0.6286666393280029\n\n\nSo, model 2 appears to perform worse than model 1, without changing any layers. The reason for this is probably that we need to add more layers to our model since our input data is now more complex (we are inputting an entire article as opposed to just a phrase from the title).\nFor model 3, let’s utilize both text and title, and also add more layers to improve the overall performance."
  },
  {
    "objectID": "posts/hw-6/index.html#model-3-text-and-title",
    "href": "posts/hw-6/index.html#model-3-text-and-title",
    "title": "Experimenting with different Keras Models to classify fake news",
    "section": "Model 3: text and title",
    "text": "Model 3: text and title\nSince we already defined the features for model 1 (title) and model 2 (text), we can simply combine these features when creating the layers for model 3.\nAlso, we want to add some more dense layers to improve the overall model performance.\nFinally, we need to add a prediction / output layer.\n\nboth_features = layers.concatenate([title_features, text_features])\nboth_features_features = layers.Dense(128, activation='relu')(both_features)\nboth_features = Dropout(0.2)(both_features)\nboth_pred = Dense(1, activation='sigmoid')(both_features)\n\n\n# inputs (same as before)\ntitle_input = layers.Input(shape=(1,), dtype=tf.string, name='title')\ntext_input = layers.Input(shape=(1,), dtype=tf.string, name='text')\n\n# features (same as before)\n# title features\ntitle_features = text_vectorize_layer(title_input) # NLP layers\ntitle_features = embedding_layer(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features) # layers to prevent overfitting\ntitle_features = Dropout(0.2)(title_features)\n# text features\ntext_features = text_vectorize_layer(text_input)\ntext_features = embedding_layer(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = Dropout(0.2)(text_features)\n\n# combine features for model3\nboth_features = layers.concatenate([title_features, text_features])\nboth_features = Dropout(0.2)(both_features)\nboth_pred = Dense(1, activation='sigmoid')(both_features)\n\nmodel3 = Model([title_input, text_input], both_pred)\nmodel3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# map to full x-tuple because we want both text and title input\nhistory3 = model3.fit(train_data.map(lambda x, y: (x, y)),\n                      epochs=11,\n                      validation_data=val_data.map(lambda x, y: (x, y)))\n\nWARNING:tensorflow:AutoGraph could not transform &lt;function &lt;lambda&gt; at 0x7b13195fc940&gt; and will run it as-is.\nCause: could not parse the source code of &lt;function &lt;lambda&gt; at 0x7b13195fc940&gt;: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda x, y: (x, y)\n\nMatch 1:\nlambda x, y: (x, y)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING:tensorflow:AutoGraph could not transform &lt;function &lt;lambda&gt; at 0x7b133435d240&gt; and will run it as-is.\nCause: could not parse the source code of &lt;function &lt;lambda&gt; at 0x7b133435d240&gt;: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda x, y: (x, y)\n\nMatch 1:\nlambda x, y: (x, y)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n\n\nWARNING: AutoGraph could not transform &lt;function &lt;lambda&gt; at 0x7b13195fc940&gt; and will run it as-is.\nCause: could not parse the source code of &lt;function &lt;lambda&gt; at 0x7b13195fc940&gt;: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda x, y: (x, y)\n\nMatch 1:\nlambda x, y: (x, y)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nWARNING: AutoGraph could not transform &lt;function &lt;lambda&gt; at 0x7b133435d240&gt; and will run it as-is.\nCause: could not parse the source code of &lt;function &lt;lambda&gt; at 0x7b133435d240&gt;: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\nMatch 0:\nlambda x, y: (x, y)\n\nMatch 1:\nlambda x, y: (x, y)\n\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nEpoch 1/11\n180/180 [==============================] - 23s 109ms/step - loss: 0.4027 - accuracy: 0.8777 - val_loss: 0.2707 - val_accuracy: 0.9467\nEpoch 2/11\n180/180 [==============================] - 19s 108ms/step - loss: 0.2203 - accuracy: 0.9569 - val_loss: 0.1909 - val_accuracy: 0.9589\nEpoch 3/11\n180/180 [==============================] - 19s 103ms/step - loss: 0.1665 - accuracy: 0.9639 - val_loss: 0.1582 - val_accuracy: 0.9600\nEpoch 4/11\n180/180 [==============================] - 20s 112ms/step - loss: 0.1401 - accuracy: 0.9681 - val_loss: 0.1395 - val_accuracy: 0.9638\nEpoch 5/11\n180/180 [==============================] - 19s 106ms/step - loss: 0.1230 - accuracy: 0.9710 - val_loss: 0.1267 - val_accuracy: 0.9660\nEpoch 6/11\n180/180 [==============================] - 21s 119ms/step - loss: 0.1092 - accuracy: 0.9731 - val_loss: 0.1167 - val_accuracy: 0.9678\nEpoch 7/11\n180/180 [==============================] - 19s 108ms/step - loss: 0.1002 - accuracy: 0.9750 - val_loss: 0.1088 - val_accuracy: 0.9691\nEpoch 8/11\n180/180 [==============================] - 18s 102ms/step - loss: 0.0923 - accuracy: 0.9755 - val_loss: 0.1021 - val_accuracy: 0.9707\nEpoch 9/11\n180/180 [==============================] - 19s 104ms/step - loss: 0.0848 - accuracy: 0.9779 - val_loss: 0.0963 - val_accuracy: 0.9720\nEpoch 10/11\n180/180 [==============================] - 18s 102ms/step - loss: 0.0792 - accuracy: 0.9789 - val_loss: 0.0913 - val_accuracy: 0.9740\nEpoch 11/11\n180/180 [==============================] - 19s 105ms/step - loss: 0.0745 - accuracy: 0.9799 - val_loss: 0.0870 - val_accuracy: 0.9753\n\n\n\"model3 = Model([title_input, text_input], both_pred)\\nmodel3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\\nmodel3.fit(train_data.map(lambda x, y: (x, y)), \\n           epochs=10, \\n           validation_data=val_data.map(lambda x, y: (x, y)))\"\n\n\n\nimport numpy as np\n# Assuming val_data is your validation dataset and it's in the same format as your training data\ntitle_val = []\ntext_val = []\nlabels_val = []\n\nfor element in val_data:\n    (title, text), labels = element\n    title_val.append(title.numpy())\n    text_val.append(text.numpy())\n    labels_val.append(labels.numpy())\n\ntitle_val = np.concatenate(title_val)\ntext_val = np.concatenate(text_val)\nlabels_val = np.concatenate(labels_val)\n\n# Model 3: Using both the title and the text\nloss3, accuracy3 = model3.evaluate([title_val, text_val], labels_val, verbose=2)\nprint(f'Model 3 Accuracy: {accuracy3}')\n\n141/141 - 2s - loss: 0.0870 - accuracy: 0.9753 - 2s/epoch - 12ms/step\nModel 3 Accuracy: 0.9753333330154419\n\n\n\nplot_history(history3)\n\n\n\n\n\n\n\n\nGreat! Looks like the strategy of adding more layers as well as looking at multiple inputs helped our accuracy jump up to 97%.\nSince model 3 performed best, let’s test it on some unseen data, and let’s further analyze these results."
  },
  {
    "objectID": "posts/hw-6/index.html#word-embedding-visualization",
    "href": "posts/hw-6/index.html#word-embedding-visualization",
    "title": "Experimenting with different Keras Models to classify fake news",
    "section": "Word embedding visualization",
    "text": "Word embedding visualization\nLet’s visualize the word embeddings from our best model.\n\n# first, we will get the weights from the embedding layer\nweights = model3.get_layer('embedding').get_weights()[0]\nvocab = text_vectorize_layer.get_vocabulary()          # obtain vocabulary\n\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nweights = pca.fit_transform(weights)\n\n\nembedding_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\nembedding_df.head()\n\n\n  \n    \n\n\n\n\n\n\nword\nx0\nx1\n\n\n\n\n0\n\n0.217996\n-1.184181\n\n\n1\n[UNK]\n9.833780\n5.256103\n\n\n2\nsaid\n3.888026\n20.351713\n\n\n3\ntrump\n7.688670\n1.631809\n\n\n4\nus\n-22.443010\n-2.284579\n\n\n...\n...\n...\n...\n\n\n1995\nsuper\n7.278611\n-2.015833\n\n\n1996\nsees\n-31.323723\n-5.284711\n\n\n1997\nfiring\n-9.978710\n-0.367837\n\n\n1998\nshift\n-6.573656\n1.414251\n\n\n1999\nreelection\n5.210635\n0.541366\n\n\n\n\n2000 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nGreat! So we have reduced our word-embeddings to 2 dimensions. And note that we only took in 2000 words to start. Let’s take a look at what this looks like visually.\n\nimport plotly.express as px\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 3,\n                 hover_name = \"word\")\n\nAfter plotting the above figure, you should get a result like this:\n\n\n\nScreenshot 2024-03-11 at 11.20.59 PM.png\n\n\nIn this word embedding visualization, words that are semantically similar are placed close together.\nSame verb with variations in usage:\nWe can see that the words - ‘lie’ - ‘lies’ - ‘lying’\nare all on the central right of the diagram. To us, this isn’t a very exciting example because these are all variations of the same exact verb. However, it is important to note that the model does not know this. Yet the model is able to determine that these are semantically similar.\nSemantically similar in context of news:\nFor a more interesting example, we can see that - ‘city’ - ‘government’\nare close together, as well. In terms of political news, it makes sense that these two words would be used within the same context. However, if we were analyzing agricultural articles, then these might not be considered so semantically similar. Something to think about!\nOutlier is not similar to anything else Finally, we have some strange words that don’t appear to be semantically similar to anything else. For example, - “I”\nis on the upper left outskirt and is not close to anything else. This makes sense, as the first-person singular pronoun does not really have any synonyms."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Experimenting with different Keras Models to classify fake news\n\n\n\n\n\n\nweek 10\n\n\nhomework6\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nAnika Misra\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizing 2D heat diffusion by experimenting with matrix-vector multiplication, JAX, Numpy, and more.\n\n\n\n\n\n\nweek 7\n\n\nhomework4\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nAnika Misra\n\n\n\n\n\n\n\n\n\n\n\n\nHow to create a simple (pink) Message Bank Webapp that submits and views messages using Python and Flask\n\n\n\n\n\n\nweek 6\n\n\nhomework3\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nAnika Misra\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use Python and Scrapy to determine movies you will like based on Legally Blonde\n\n\n\n\n\n\nweek 4\n\n\nhomework2\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nAnika Misra\n\n\n\n\n\n\n\n\n\n\n\n\nUsing SQL Querying to compare different countries’ climate trends and also to compute year-long trends of temperature increases\n\n\n\n\n\n\nweek 3\n\n\nhomework1\n\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\nAnika Misra\n\n\n\n\n\n\n\n\n\n\n\n\nHW0\n\n\n\n\n\n\nweek 2\n\n\nhomework0\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\nAnika Misra\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nart\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nAnika Misra\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nAnika Misra\n\n\n\n\n\n\nNo matching items"
  }
]