[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/hw-0/index.html",
    "href": "posts/hw-0/index.html",
    "title": "HW0",
    "section": "",
    "text": "In today’s dataset from Github, I will be explaining how to create a simple visualization from the Palmer Penguins dataset. There are many features in this dataset, such as species, region, island, egg stage, culmen features, body mass, gender, and delta 15 N features. However, the question we are trying to answer today is:\n\n\nLet’s get started."
  },
  {
    "objectID": "posts/hw-0/index.html#improvements-and-next-steps",
    "href": "posts/hw-0/index.html#improvements-and-next-steps",
    "title": "HW0",
    "section": "Improvements and next steps:",
    "text": "Improvements and next steps:\n\ncreate DataFrames of each island and species to determine the exact range of body masses\nGet more specific statistics and exact numbers as opposed to relying on visualizations\nGet more data from Chinstrap and Gentoo penguins that comes from other islands\n\nI hope you learned something with today’s blog post! Have an ice day.\n\n\n\nHave an ice day.png\n\n\nImage sources: Ice\nPenguin"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in my Quarto blog. Just testing things out. Here is a painting I made two days ago."
  },
  {
    "objectID": "posts/hw-3/index.html",
    "href": "posts/hw-3/index.html",
    "title": "How to create a simple (pink) Message Bank Webapp that submits and views messages using Python and Flask",
    "section": "",
    "text": "Today we will be creating a webapp using Python and Flask to submit and view messages. A “webapp” is exactly what it sounds like: it’s like an application (or, app) you can deploy it on the internet (or, web). In today’s blog, the app won’t actually be published on the internet, but we will still be accessing it on our local browser.\nI will be describing how to set up this project in detail. If you want more details, a link to the project repository is here:\nhttps://github.com/anikamisra/pic16b-hw3-webapp/"
  },
  {
    "objectID": "posts/hw-3/index.html#function-1-get_message_db",
    "href": "posts/hw-3/index.html#function-1-get_message_db",
    "title": "How to create a simple (pink) Message Bank Webapp that submits and views messages using Python and Flask",
    "section": "Function 1: get_message_db",
    "text": "Function 1: get_message_db\nThis function handles database creation for messages. First, make sure to import the necessary packages into app.py:\n\nfrom flask import Flask, request, render_template\nimport sqlite3\nfrom flask import g\nimport random\n\nNow, let’s create the method. The way this function works is: 1. First it will check if the SQL message database, called message_db, exists. 2. If the database doesn’t exist yet, then it will create a new one. 3. Open a connection to the message database and store it in g.message_db 4. Create a cursor with the database object (used for SQL commands) 5. Execute SQL command to create a “messages” table if it does not already exist 6. Return the database connection.\nHere’s a look at the code:\n\ndef get_message_db():\n    \"\"\"\n    handles database creation for messages  \n    \"\"\"\n    # check if message_db exists\n    if 'message_db' not in g:\n        # connect database \n        g.message_db = sqlite3.connect('messages_db.sqlite')\n    cursor = g.message_db.cursor()\n\n    # check for messages table  \n    cursor.execute('CREATE TABLE IF NOT EXISTS messages (id INTEGER PRIMARY KEY, handle TEXT, message TEXT)')\n    return g.message_db\n\nGreat! We’ve created our first method. But, what will this functio be used with? Now, let’s define the second method in our aplication script file:"
  },
  {
    "objectID": "posts/hw-3/index.html#function-2-insert_message",
    "href": "posts/hw-3/index.html#function-2-insert_message",
    "title": "How to create a simple (pink) Message Bank Webapp that submits and views messages using Python and Flask",
    "section": "Function 2: insert_message",
    "text": "Function 2: insert_message\nThis function will handle user insertion of messages into the database.\nHere is how it works: 1. Connect database by calling get_message_db() 2. Connect cursor 3. Execute SQL command to insert message and user id into messages table 4. Run commit() function to ensure the row insertion has been saved 5. Close database connection\nHere is what the code looks like:\n\ndef insert_message(request):\n    \"\"\"\n    insert message into database after user submission \n    \"\"\"\n    # connect database \n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute('INSERT INTO messages (handle, message) VALUES (?, ?)', (request.form['user'], request.form['message']))\n    # save \n    db.commit()\n    # close the database connection \n    db.close()\n\nSo, this function works by calling the previously defined function, get_message_db(). Furthermore, we can see how it specifically inserts the username and the message of the person who posted it. Don’t worry too much about “resquest.form” for now - we will get into this later."
  },
  {
    "objectID": "posts/hw-3/index.html#function-3-random_messages",
    "href": "posts/hw-3/index.html#function-3-random_messages",
    "title": "How to create a simple (pink) Message Bank Webapp that submits and views messages using Python and Flask",
    "section": "Function 3: random_messages",
    "text": "Function 3: random_messages\nThe last non-route function we will define in our application instance file is the random_messages function. This function is used to generate 5 random messages that our in our database (or, fewer, if there aren’t 5 submitted messages yet). We will call this function when we view the messages in our webapp. Here is how it works: 1. Connect to database by calling Function # 1 2. Connect cursor and execute SQL command of selecting all messages 3. Define a cursor object, messages, that selects all messages and usernames submitted 4. Close database connection 5. Utilize random library to generate n messages from “messages” cursor object. If there are less than n messages in the database, then select however many are in the database.\nLet’s take a look at the code:\n\ndef random_messages(n):\n    \"\"\"\n    returns n (or fewer) random messages for viewing \n    \"\"\"\n    # connect database and get messages\n    db = get_message_db()\n    cursor = db.cursor()\n    cursor.execute('SELECT * FROM messages')\n    messages = cursor.fetchall()\n    # close connection \n    db.close()\n    # return n messages (or, fewer if there aren't n messages)\n    return random.sample(messages, min(n, len(messages)))\n\nGreat! Now, we have defined our non-route functions in the application, let’s get onto the route functions."
  },
  {
    "objectID": "posts/hw-3/index.html#function-4-home-page",
    "href": "posts/hw-3/index.html#function-4-home-page",
    "title": "How to create a simple (pink) Message Bank Webapp that submits and views messages using Python and Flask",
    "section": "Function 4: home page",
    "text": "Function 4: home page\nThis function will render the HTML template to create our home page. We want our default page to be a simple page that allows the user to navigate to wherever they want to go. So, we will only use the ‘GET’ method here.\n\n# url route function for home page     \n@app.route('/', methods=['GET'])\ndef home():\n    # render submit.html page \n    return render_template('base.html')\n\nWe specify the method as ‘GET’ because we will not be posting any data here. It is simply the home page that takes us to either submitting, or viewing a message. This template is the “base” that the other HTML pages, which are view.html and submit.html, will “inherit” from. Here is what base.html looks like:\n\nbase.html\n\n\n\nScreenshot 2024-02-14 at 6.54.33 PM.png\n\n\nAs we can see, there are only two links on the page that take us to “submit” and “messages”. Now, let’s define the route functions for these two pages. Furthermore, we can incorporate the style.css file that we want in order to customize our website. In my style.css file, I made the background of my webpage pink. Here is what the home page looks like:\n\n\n\nScreenshot 2024-02-14 at 6.13.03 PM.png\n\n\n(We will learn more about how to get this running on your local browser in Section 3.)\nSo pink, so beautiful! (Note, I made it pink and changed the font in “style.css”.)\nNow, let’s create the route functions to render “submit.html” and “view.html”, which will allow us to actually use those cute links that are on our home page."
  },
  {
    "objectID": "posts/hw-3/index.html#function-5-submit-page",
    "href": "posts/hw-3/index.html#function-5-submit-page",
    "title": "How to create a simple (pink) Message Bank Webapp that submits and views messages using Python and Flask",
    "section": "Function 5: Submit page",
    "text": "Function 5: Submit page\nIn this function, we will render the submit.html template, which will allow us to actually submit messages! Yay! In order to do this, we will use both the ‘GET’ and ‘POST’ methods, since we need to respond to HTTP requests and also transmit the data of the submitted message. Here is how it works:\n\nSet message and user to none before anything has been inputted.\nOnce the user has submitted the form (“post” button has been clicked), send in the data as an HTTP request.\n\n\nTo do this, we use Flask’s “request.form”, which is a type of special dictionary used to store these HTTP requests\n\n\nCall the previously defined insert_message function with the request as an argument.\n\n\nRemember how we used “(request.form[‘user’], request.form[‘message’]) in the SQL command? This is where it comes from.\n\n\nRender submit.html template. We will “pass in” the user’s name and message because it will be used in the html page.\n\nHere is what the function should look like:\n\n# url route function for submit page  \n@app.route('/submit', methods=['GET', 'POST'])\ndef submit():\n    message = None\n    user = None\n    # handle user posting \n    if request.method == 'POST':\n        user = request.form['user']\n        message = request.form['message']\n        # call insert_message function \n        insert_message(request)\n    return render_template('submit.html', user=user, message=message)\n\nWe will pass in user into submit.html because we want to thank the user for their submission. Here is what the submit.html page should look like:\n\nsubmit.html\n\n\n\nScreenshot 2024-02-14 at 6.56.20 PM.png\n\n\nAs we can see, submit.html extends the base.html page, and thanks the user for their submission. It also stores a box for message and user. We will see what this looks like in Section 3. As of right now, the “message” variable is not used, but it is still handy to pass it in in case we want to customize our submit.html function later and use that variable."
  },
  {
    "objectID": "posts/hw-3/index.html#function-6-view-messages-page",
    "href": "posts/hw-3/index.html#function-6-view-messages-page",
    "title": "How to create a simple (pink) Message Bank Webapp that submits and views messages using Python and Flask",
    "section": "Function 6: View messages page",
    "text": "Function 6: View messages page\nWe are almost done with our functions! Finally, let’s create a function to render the view.html page. This will allow us to see 5 (or less) randomly generated messages that have been inputted.\nOnce again, we are only using the ‘GET’ method here because we are not transmitting any data - only receiving the HTTP requests. The “messages” variable utilizes the previously defined random_messages() function to obtain and store 5 random messages that are in our messages table in the database. Then, it renders the view.html page and passes in the 5 obtained messages.\nHere is what the function looks like:\n\n#  url route function for view messages page \n@app.route('/messages', methods=['GET'])\ndef messages(): \n    # generate 5 random messages to view \n    messages = random_messages(5)\n    return render_template('view.html', messages=messages)\n\nBut how do the messages get used? Let’s take a look at view.html:\n\nview.html\n\n\n\nScreenshot 2024-02-14 at 6.56.48 PM.png\n\n\nAs we can see, we display the 5 random messages we obtained in an ordered list.\nEvery time we click “View messages” on our webapp, it will rerun the url route function and obtain 5 NEW randomly selected messages.\nNow, it’s time to see all this in action! In the next section, I will use screenshots to show how the Message Bank webapp really works!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This python code is known as one of the most complex pieces of code ever written. If you run this in your IDE, it might blow your mind:\n\nprint(\"hello world\") \n\nhello world"
  },
  {
    "objectID": "posts/hw-2/index.html",
    "href": "posts/hw-2/index.html",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "",
    "text": "Welcome! In today’s post, I will be describing how to build a webscraper to scrape the TMDB movie database. By the end of this post, you should understand how to build a webscraper, and you should be able to use this webscraper for the movie of your choice to find recommendations based on it. For my movie, I’m going to choose Legally Blonde.\n\n\n\nSegmentLocal\n\n\nImage source\nWhat a girlboss!!"
  },
  {
    "objectID": "posts/hw-2/index.html#part-0a-initializer",
    "href": "posts/hw-2/index.html#part-0a-initializer",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 0A: Initializer",
    "text": "Part 0A: Initializer\nJust like Elle Woods had to study hard for her LSAT before she got into Harvard, we all have to start somewhere. Before webscraping the TMDB website, we have to build our scraper. In this section, I will explain how to build the scraper and how the scraper works. Then, in part B, I will explain where this TmdbSpider class should go and how to actually set up the scraper in your local computer. So for now, just follow along with the explanation, and we will set everything up in Part B.\nWe begin this webscraper by importing the necessary packages, defining a class, and constructing the initializer.\nWe will name the class TmdbSpider since we are creating a webscraper for the TMDB database. It will inherit the class scrapy.Spider.\nIn the initializer, we will accept a “subdir” string argument. This argument is a string that contains the last part of the url the movie we want to scrape. It should look something like this:\n“8835-legally-blonde”\nThis url ending can be found by looking at the url of the movie we want to scrape on the TMDB website.\nThen, we will create an instance variable called start_urls that builds the complete url for the movie to scrape.\n\n# import necessary packages \nimport scrapy \n\n# define a class that inherits scrapy.Spider, the base class for spiders \nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider' # name our spider \n    def __init__(self, subdir=None, *args, **kwargs):\n        \"\"\" \n        Class constructor for the TmdbSpider class. \n        \n        Args: \n            self (TmdbSpider class instance): instance of the TmdbSpider class. \n            subdir (string): String for the subdirectory of the movie we want to start with. This string can be found in the movie url \n            *args: additional arbitrary keyword arguments \n            **kwargs: additional keyword arguments  \n            \n        \"\"\" \n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"] # build the complete url from the subdirectory"
  },
  {
    "objectID": "posts/hw-2/index.html#part-1a-parse-method",
    "href": "posts/hw-2/index.html#part-1a-parse-method",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 1A: Parse Method",
    "text": "Part 1A: Parse Method\nThe first class method we will define is the parse() function. This is how the parse() function works:\n\nFirst, obtain the url of the full cast and crew. This will be done by “hardcoding”, aka, manually adding a “cast” string onto our start_urls instance variable.\nCall the parse_full_credits() method. This will be done by specifying a callback argument to a yielded scrapy request.\n\nThis method assumes we start on a movie page, and it should navigate us to a “full cast and crew” page for that specific movie. Then, it calls the next parse function, which we will define in part 2.\n\ndef parse(self, response): \n        \"\"\"\n        Assumptions: \n            Assume we are starting on a movie page. \n        Effects: \n            Navigate to a \"full cast and crew\" page for that specific movie. \n            Data outputs: A yielded scrapy request calling parse_full_credits() method with the harcoded \"full cast and crew\" url \n        \"\"\"\n        \n        # first, obtain the url of full cast and crew (hardcoded)\n        # response.url is a built-in method that gives us current url of response\n        cast_url = response.url + '/cast/'  \n          \n        \n        # parse_full_credits method is called by specifying callback argument to yielded scrapy request\n        yield scrapy.Request(url = cast_url, callback = self.parse_full_credits)"
  },
  {
    "objectID": "posts/hw-2/index.html#important",
    "href": "posts/hw-2/index.html#important",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Important:",
    "text": "Important:\nPlease note that each of the parse methods we are defining are all part of the TmdbSpider class!!!\n\nAnyways…\nWhat just happened in the last line? Well, in the “yield scrapy.Request” line, we first tell scrapy to fetch this url. After it fetches the “full cast and crew” url, we obtain the response object. This response object is automatically passed in as the second argument to the callback function, parse_full_credits. And, remember from PIC16A that “self” is always passed in as the first argument. This is why parse_full_credits function has no arguments specified.\nparse_full_credits: 1. First argument: self (automatic) 2. Second argument: response object from cast_url (automatic)\nWhat does the parse_full_credits() method do? Well, let’s define it now!"
  },
  {
    "objectID": "posts/hw-2/index.html#part-2a-parse-full-credits-method",
    "href": "posts/hw-2/index.html#part-2a-parse-full-credits-method",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 2A: Parse full credits Method",
    "text": "Part 2A: Parse full credits Method\nThe second class method we will define is parse_full_credits(). This function takes in two arguments: a reference to the current instance of the class “self”, and the response object generated from the previous parse method. It assumes we start on the “full cast and crew” wegpage for a specific movie. Here is how it works:\n\nLocate the “cast” table of the webpage.\n\n\nTo do this, we will search for an h3 element containing “Cast” using Xpath.\nWe must ensure we are only in the cast table, or else we might get non-actors in our list (aka, members from the production crew).\n\n\nObtain the list of cast members.\n\n\nSelect the first “ordered list” (ol) element that appears after the h3 element. We can do this using “following-sibling::ol[1]”.\nNote that the ordered list element is not a “child” of the h3 element, which is why we use the “following-sibling” expression.\n\n\nObtain the actor urls.\n\n\nUsing the xpath object of cast members, navigate into the list element that contains a div element with a class attribute containing “info”.\nThen, look for a &lt;&gt; tag after the div class. This &lt;&gt; tag may or may not be the direct descendent of the div element, which is why we use the “descendant-or-self” line.\nFinally, navigate into the &lt;&gt; tag and grab the url inside it. Use the “getall()” method to obtain all such links.\n\n\nYield scrapy requests for each actor url, using the parse_actor_page method as a callback function.\n\nHere is what the function should look like.\n\ndef parse_full_credits(self, response): \n        \"\"\"\n        Assumptions: \n            Assume we are starting on a \"full cast and crew\" page for one movie. \n        Effects: \n            Navigate to each actor's page for the specific movie (note - ACTOR - not any other crew member!) \n            Data outputs: A yielded scrapy request calling parse_actor_page() method for every single actor's link in this webpage. \n        \"\"\"\n        \n        # Step 1: Locate cast table of webpage (h3 tag contains \"Cast\")  \n        # Step 2: Create an xpath object from this \"Cast\" table, called table1 \n        table1 = response.xpath('//h3[contains(., \"Cast\")]/following-sibling::ol[1]')\n        \n        # Step 3: Obtain all the actor urls by finding the proper link for each actor  \n        urls_actors = table1.xpath('.//li//div[contains(@class, \"info\")]/descendant-or-self::*/p/a/@href').getall()\n        # notice that we use getall() method to obtain ALL actor urls \n        \n        # Step 4: Yield scrapy requests for each actor url \n        for link in urls_actors: \n            yield scrapy.Request(url = response.urljoin(link), callback = self.parse_actor_page) \n            # we use urljoin as opposed to simple string concatonation to ensure that relative urls are formed properly \n\nHere is what the HTML element looks like. In this HTML element, we are trying to obtain the second “a href=”/person/368-reese-witherspoon” text to get the link for the actor page. We specifically try to get the second one, and not the first one, because each actor has two links and we do not want more than one link per actor.\n\n\n\nScreenshot 2024-02-05 at 1.31.27 PM.png\n\n\nIn our function, just like before, we do not put any arguments into the parse_actor_page() function because the second argument is automatically the response object fetched by the url we specified, and the first argument is automatically “self”. But what is the parse_actor_page() function? Let’s define it right now!"
  },
  {
    "objectID": "posts/hw-2/index.html#part-3a-parse-actor-page",
    "href": "posts/hw-2/index.html#part-3a-parse-actor-page",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 3A: Parse actor page",
    "text": "Part 3A: Parse actor page\nThe final class method we will define is the parse_actor_page() method. This function takes in two arguments, (1) a reference to the current instance of the class “self”, and (2) the response object generated from the previous class method we defined. It assumes we start on an actor’s webpage. It will yield a dictionary object for each actor and each movie they played in. Here is how it works:\n\nGet the actor’s name: It will be on the very top of the webpage in the &lt;&gt; title element.\nLocate the “acting” table in the actor’s webpage and create an xpath object from it called “table”.\n\n\nWe do this by first finding the h3 element with text element “Acting”, and then we select the element that occurs directly after it.\nThis is why we use the “following-sibling” code.\n\n\nObtain each movie / tv show name from the acting “table”.\n\n\nThis is done by navigating to the &lt;&gt; element within the acting “table”.\n\n\nYield a key-pair value containing {actor_name, movie_or_TV_name} for every movie / tv show that the actor played in.\n\n\ndef parse_actor_page(self, response): \n        \"\"\"\n        Assumptions: \n            Assume we are starting on an actor's webpage. \n        Effects: \n            Does not navigate to any new webpages.  \n            Data outputs: A yielded dictionary, each key-value pair containing the movie that an actor acted in along with the actor's name. \n        \"\"\"\n        \n        # Step 1: obtain the actor's name from the Title of the webpage \n        actor_name = response.css('h2.title &gt; a::text').get()\n        \n        # Step 2: Locate the acting table in the webpage, and select the element that occurs immediately after it \n        table = response.xpath('//h3[text()=\"Acting\"]/following-sibling::table[1]')\n        \n        # Step 3: Obtain the relevant movie / tv show names from each acting \"table\"  \n        movie_names = table.css('td.role.true.account_adult_false.item_adult_false &gt; a.tooltip &gt; bdi::text').getall()\n        # notice that we use \"getall()\" method to obtain ALL movie / tv show names  \n        \n        # Yield a key-value pair for each movie / tv show in this actor's webpage \n        for movie_or_TV_name in movie_names: \n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name} \n\nHere is what the HTML element looks like. In this HTML element, we are trying to obtain the text within the &lt;&gt; element because that gives us the movie / tv show name.\n\n\n\nScreenshot 2024-02-05 at 5.19.29 PM.png"
  },
  {
    "objectID": "posts/hw-2/index.html#part-0b-setting-up-scraper",
    "href": "posts/hw-2/index.html#part-0b-setting-up-scraper",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 0B: Setting up scraper",
    "text": "Part 0B: Setting up scraper\nNow that we know how to build our TMDB Spider, let’s actually set it up and use it to find recommendations based on Legally Blonde. The first step is to set up your local machine for webscraping. Here are the first few steps:\nIn your terminal, 1. Activate the Python environment of your choice 2. Navigate into the directory where you want your scraper files to be\n3. Run the following lines in your terminal:\nscrapy startproject TMDB_scraper cd TMDB_scraper\nThis will create a lot of folders and files. Do not worry about these. Here are the next steps: 1. Inside your spider directory, add a file called “tmdb_spider.py”. Inside this file, write out the tmdbSpider class we just defined. 2. In your settings.py file, add this line: “USER_AGENT = ‘Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0’”. This will ensure we do not run into any 403 errors."
  },
  {
    "objectID": "posts/hw-2/index.html#part-1b-running-the-scraper",
    "href": "posts/hw-2/index.html#part-1b-running-the-scraper",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 1B: Running the scraper",
    "text": "Part 1B: Running the scraper\nNow, we will run our scraper for Legally Blonde. The TMDB link for Legally Blonde is:\nhttps://www.themoviedb.org/movie/8835-legally-blonde\nSo, our “subdir” argument should be 8835-legally-blonde. Navigate into the directory where you want your results file to be, and run the following line in your terminal:\nscrapy crawl tmdb_spider -o results.csv -a subdir=8835-legally-blonde\nThis line will webscrape the Legally Blonde database using the tmdb spider class we just defined. The “subdir” argument is passed into the very first “parse” method, and the webscraper navigates to the appropriate webpages from there."
  },
  {
    "objectID": "posts/hw-2/index.html#part-2b-accessing-and-analyzing-results.csv-file",
    "href": "posts/hw-2/index.html#part-2b-accessing-and-analyzing-results.csv-file",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 2B: Accessing and analyzing results.csv file",
    "text": "Part 2B: Accessing and analyzing results.csv file\nAfter running this line in your terminal, wait till you see a message that the Spider has closed in your terminal. Then, in the folder that you are currently in, there should be a file called “results.csv”. This should contain two columns: 1. Actor name 2. Movie name\nNow, let’s analyze these results to build a recommender system.\nFirst, let’s import this results.csv file and turn it into a pandas dataframe for easy analysis.\n\n# import pandas \nimport pandas as pd\n\n# import results csv file as dataframe \nresults = pd.read_csv(\"results.csv\") \n\n# preview results dataframe\nresults.head()\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nReese Witherspoon\nTiny Trailblazers\n\n\n1\nReese Witherspoon\nTracy Flick Can't Win\n\n\n2\nReese Witherspoon\nGreat Performers: 9 Kisses\n\n\n3\nReese Witherspoon\nYou’re Cordially Invited\n\n\n4\nReese Witherspoon\nLegally Blonde 3\n\n\n\n\n\n\n\nGreat! Let’s see how many actors were in the original “Legally Blonde” movie, and how many total actors and movie / TV show names there are.\n\nactors_in_legally_blonde = results[results['movie_or_TV_name'] == 'Legally Blonde']['actor'].unique()\nmovies_total = results['movie_or_TV_name'].unique()\n\nprint(\"All the actors in Legally Blonde: \\n\", actors_in_legally_blonde, \"\\n\")\n\nprint(\"All the movies that are in the results file: \\n\", movies_total, \"\\n\")\nprint(\"Number of movies / TV shows in the results file: \\n\", results['movie_or_TV_name'].nunique())\n\nAll the actors in Legally Blonde: \n ['Reese Witherspoon' 'Lily' 'Kelly Driscoll' 'Sasha Barrese' 'Moonie'\n 'Kennedy Stone' 'Elizabeth Matthews' 'Richard Hillman' 'John Kapelos'\n 'Patricia Kimes' 'Jodi Harris' 'Nectar Rose' 'Terence Michael'\n 'John Cantwell' 'Ondrea de Vincentis' 'Chaney Kley' 'Melissa Anne Young'\n 'Brody Hutzler' 'Lacey Beeman' 'Jason Christopher' 'Lisa K. Wyatt'\n 'Corinne Reilly' 'Doug Spinuzza' 'Niklaus Lange' 'Victoria Mahoney'\n 'Tane McClure' 'David Moreland' 'Allyce Beasley' 'Kevin Cooney'\n 'Cici Lau' 'Natalie Barish' 'Lisa Arch' 'Francesca P. Roberts'\n 'Kimberly McCullough' \"Shannon O'Hurley\" 'Ted Rooney' 'Kelly Nyks'\n 'Samantha Lemole' 'Michael B. Silver' 'Ted Kairys' 'Bruce Thomas'\n 'Meredith Scott Lynn' 'Linda Cardellini' 'Raquel Welch' 'Greg Serano'\n 'Osgood Perkins' 'Wayne Federman' 'Jessica Cauffiel' 'Alanna Ubach'\n 'Victor Garber' 'James Read' 'Jennifer Coolidge' 'Holland Taylor'\n 'Matthew Davis' 'Selma Blair' 'Luke Wilson' 'Ali Larter'] \n\nAll the movies that are in the results file: \n ['Tiny Trailblazers' \"Tracy Flick Can't Win\" 'Great Performers: 9 Kisses'\n ... 'Giving It Up' 'Varsity Blues' 'Just Shoot Me!'] \n\nNumber of movies / TV shows in the results file: \n 1775"
  },
  {
    "objectID": "posts/hw-2/index.html#part-3b-building-our-recommender-system",
    "href": "posts/hw-2/index.html#part-3b-building-our-recommender-system",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 3B: Building our recommender system",
    "text": "Part 3B: Building our recommender system\nThere are 1775 movies/TV shows to choose from! How do we know which one we will like best? One way to do this is to see which movies / TV shows have the highest number of shared actors with Legally Blonde. Then, we can create a visualization with the top 10 movies / TV shows and the number of shared actors they contain.\nLet’s make this visualization pink, Elle Woods’ favorite color!!\n\n\n\nImage 2-5-24 at 6.40 PM.jpg\n\n\nImage source\n\n# import matplotlib \nimport matplotlib.pyplot as plt \n\n# create a new dataframe for shared actors \nsame_actors = results[results['actor'].isin(actors_in_legally_blonde) & (results['movie_or_TV_name'] != 'Legally Blonde')] \n\n# find out how many shared actors per movie \nsame_actors_value = same_actors['movie_or_TV_name'].value_counts()\n\n# create a new dataframe with top 10 movies / TV shows \ntop_movie_TV_show = same_actors_value.nlargest(10)\n\n# plot bar chart accordingly \nplt.figure(figsize=(10, 6))\n# make sure the color is pink! \ntop_movie_TV_show.plot(kind='bar', color='pink')\nplt.title('Most recommended movies / TV shows for you based on Legally Blonde')\nplt.xlabel('Movie / TV Show Name')\nplt.ylabel('Number of Shared Actors')\nplt.show()"
  },
  {
    "objectID": "posts/hw-2/index.html#part-4b-summary-and-takeaways",
    "href": "posts/hw-2/index.html#part-4b-summary-and-takeaways",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 4B: Summary and takeaways",
    "text": "Part 4B: Summary and takeaways\nWow! Now we have a visualization for a recommender system based on Legally Blonde. I am shocked there are so many true crime shows on there! I guess Reese Witherspoon really stuck to the “Legal” movie genre. Good for her! She’s such a slay girlboss queen.\nSomething else that is pretty cool about this visualization is that it not only shows you which movies and TV shows are recommended for you, but at which level they are recommended for you based on the number of actors.\nNote that you can do this for any movie of your choice using the scraper we built!\n\nThanks for reading!! And remember: Whoever said orange is the new pink was seriously disturbed."
  },
  {
    "objectID": "posts/hw-4/index.html",
    "href": "posts/hw-4/index.html",
    "title": "Optimizing 2D heat diffusion by experimenting with matrix-vector multiplication, JAX, Numpy, and more.",
    "section": "",
    "text": "Today we will be simulating 2D heat diffusion in various ways. We will be experimenting with four different functions that are predefined in my heat_equation.py file. I will show what each function looks like and explain how it works before viewing the results.\nThe first step is to make sure to import all the necessary packages. There are a lot!\n\nimport jax\nimport jax.numpy as jnp\nimport time\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom jax.experimental import sparse\nimport inspect\n\nAlright, let’s go ahead and dive right in."
  },
  {
    "objectID": "posts/hw-4/index.html#welcome",
    "href": "posts/hw-4/index.html#welcome",
    "title": "Optimizing 2D heat diffusion by experimenting with matrix-vector multiplication, JAX, Numpy, and more.",
    "section": "",
    "text": "Today we will be simulating 2D heat diffusion in various ways. We will be experimenting with four different functions that are predefined in my heat_equation.py file. I will show what each function looks like and explain how it works before viewing the results.\nThe first step is to make sure to import all the necessary packages. There are a lot!\n\nimport jax\nimport jax.numpy as jnp\nimport time\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom jax.experimental import sparse\nimport inspect\n\nAlright, let’s go ahead and dive right in."
  },
  {
    "objectID": "posts/hw-4/index.html#part-2-matrix-vector-mulitplication-with-jax-and-sparse-matrices",
    "href": "posts/hw-4/index.html#part-2-matrix-vector-mulitplication-with-jax-and-sparse-matrices",
    "title": "Optimizing 2D heat diffusion by experimenting with matrix-vector multiplication, JAX, Numpy, and more.",
    "section": "Part 2: Matrix-vector mulitplication with JAX and sparse matrices",
    "text": "Part 2: Matrix-vector mulitplication with JAX and sparse matrices\nOne of the issues with using A in Part 1 is that we waste a lot of time computing the 0s during the matrix vector multiplication. Luckily, Jax has a solution for this called the batched coordinate format (BCOO), which exploits the 0s and increases computing time. Saving space on the 0’s is what turns A into a “sparse” matrix. If we do not use Jax’s batched coordinate format, then A is a “dense” matrix. In order to utilize it, let’s turn A into a “sparse” matrix. Here is how we can do this:\n\nfrom heat_equation import get_sparse_A\nprint(inspect.getsource(get_sparse_A))\n\ndef get_sparse_A(N): \n    \"\"\"\n    Creates finite difference matrix that is sparse. \n    Args:\n        N: the square root of the row and column sizes of A \n    Returns: \n        A: the N^2 x N^2 finite difference matrix A that is in sparse format \n    \"\"\"\n    n = N * N\n    # create proper entries for 2D heat diffusion in matrix-vector multiplication\n    # utilize jax.numpy in order to be compatible with jax's XLA compiler  \n    diagonals = [-4 * jnp.ones(n), jnp.ones(n-1), jnp.ones(n-1), jnp.ones(n-N), jnp.ones(n-N)] \n    \n    # make the other rows and columns 0 \n    diagonals[1] = diagonals[1].at[(N-1)::N].set(0)\n    diagonals[2] = diagonals[2].at[(N-1)::N].set(0)\n    \n    # create A \n    A = jnp.diag(diagonals[0]) + jnp.diag(diagonals[1], 1) + jnp.diag(diagonals[2], -1) + jnp.diag(diagonals[3], N) + jnp.diag(diagonals[4], -N)\n    \n    # use batched coordinate format from Jax to use less space in the matrix \n    A_sp_matrix = sparse.BCOO.fromdense(A)\n    return A_sp_matrix\n\n\n\nRemember the “@jax.jit” decorator at the top of our advance_time_matvecmul() function? We want the matrix multiplier to actually utilize jax’s XLA compiler, aka, Accelerated Linear Algebra.\nSo, we are using the “jit-ed” version of advance_time_matvecmul(). In order to do this, we need to also use jax.numpy operations when we create the sparse version of A.\nFinally, we convert A to a sparse matrix in order to save space on the 0s. This is the matrix we pass in to advance_time_matvecmul() when we run our heat simulation diffusion.\nLet’s go ahead and run this simulation using the sparse matrix, and see if it improves the speed.\n\n# redefine u0 (since it was used in our last simulation) \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n# get sparse matrix format of A \nA_sparse = get_sparse_A(N)\n\n# run the simulation \nsolutions = dict()\nstart_time = time.time()\n\nfig, axs = plt.subplots(3,3)\nfor i in range(2700): \n    u0 = advance_time_matvecmul(A_sparse, u0, epsilon) \n    # store immediate solution \n    solutions[i] = u0 \nend_time = time.time()\n# calculate total time for simulation, not including visualization \ntotal_time = end_time - start_time\n\n# visualize \nfor i in range(2700): \n    if (i + 1) % 300 == 0: \n        ax = axs[i // 300 // 3, i // 300 % 3]\n        ax.imshow(solutions[i])\n        ax.set_title(f'Iteration {i+1}')\n\nprint(f\"time it took: {total_time:0.2f} sec\")\n\ntime it took: 0.75 sec\n\n\n\n\n\n\n\n\n\nWow! We jumped from 168 seconds to 0.75 seconds. That’s almost 230x faster! Talk about an improvement. Looks like utilizing jax’s BCOO format, along with utilizing its XLA compiler really speeds things up. This is why everyone loves GPUs. But what if we don’t even need a finite difference matrix, A? That brings us to the next part…."
  },
  {
    "objectID": "posts/hw-1/index.html",
    "href": "posts/hw-1/index.html",
    "title": "Using SQL Querying to compare different countries’ climate trends and also to compute year-long trends of temperature increases",
    "section": "",
    "text": "Welcome!\nToday we will be creating several interactive plots from the NOAA climate dataset. However, because this dataset is extremely large, we will be taking advantage of SQL querying, which allows us to select the data we need without uploading the entire file and using up too much memory.\n\n\nSection 1: Creating the database\nFirst, we will upload all necessary packages.\n\nimport sqlite3\nimport pandas as pd\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nNow let’s enter the relevant file names.\n\ncountries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ntemps_url = \"temps.csv\"\nstation_url = \"station-metadata.csv\"\n\n# add link to download other tables \n\nNow we will create a database called my_database.db.\n\nconn = sqlite3.connect(\"database.db\") # this creates a database in current directory called my_database.db\n\nNow we are ready to add our first table to our database. The entire purpose of using SQL querying is that we avoid uploading the entire table in order to prevent overusage of RAM. Because the temperatures dataset is extremely large, for this reason, we will upload it chunk by chunk. And, because the other datasets are smaller, we can just upload them all at once.\nLet’s set the chunksize of the temps dataframe to 100,000. With the chunksize keyword specified, the read_csv function of pandas creates an iterator of the temps dataframe.\n\nCHUNKSIZE=100000\ntemps_df_iter = pd.read_csv(temps_url, chunksize=CHUNKSIZE)\ntemps_df = temps_df_iter.__next__()\ntemps_df.shape\n\n(100000, 14)\n\n\nAs we can see, the temps dataframe indeed has only 100,000 rows! Let’s finish uploading the other files and then preview everything.\n\ncountries_df = pd.read_csv(countries_url)\ncountries_df.shape\n\n(279, 3)\n\n\n\nstations_df = pd.read_csv(station_url) \nstations_df.shape\n\n(27585, 5)\n\n\nAs we can see, the stations and countries files are not nearly as large as the temperatures file. Now, let’s preview what each table looks like so we are familiar with the column names and how to query accordingly.\n\ntemps_df.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\n\ncountries_df.head()\n\n\n\n\n\n\n\n\nFIPS 10-4\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa\n\n\n\n\n\n\n\n\nstations_df.head()\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\n\n\n\n\n\n\n\nGreat! We are finally ready to add tables to our database. We will add three tables, one for temperatures, one for stations, and one for countries.\nFirst, let’s create a function to clean up the temperatures dataframe a bit.\n\ndef prepare_df(df):\n    \"\"\"\n    Function that cleans and reorganizes the temperatures dataframe. \n    \n    args: \n        df (pandas.DataFrame): the dataframe to organize, from the temperatures file of the NOAA database \n    returns: \n        df (pandas.DataFrame): the reorganized dataframe \n    \"\"\"\n    \n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\nNow let’s add the temperatures dataframe to our SQL database as a table by working with only 100,000 rows at a time. We are able to do this thanks to the iterator we created.\n\nfor i, df in enumerate(temps_df_iter):\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\nNow, let’s add the other two dataframes as tables to our database.\n\nstations_df.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\ncountries_df.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n279\n\n\nLet’s make sure that everything was added to our database successfully. Running the following code will allow us to see what is currently in our database.\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\nAwesome! We have three tables, one for temps, one for stations, and one for countries. Now, we are done with our database connection, so it is good practice to close it.\n\nconn.close()\n\n\n\nPart 2: Querying our first database\nLet’s now use our database to perform an SQL query that returns a pandas dataframe. The beauty of this is that we do not have to have the entire dataframe in our RAM. To do this, we will use the query_climate_database() function which I have already defined. Let’s see what it looks like below:\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month): \n    \"\"\"\n    Function which performs SQL query based on country, year beginning, year end, and month. \n    \n    args: \n        db_file (str): string for database name \n        country (str): string for country \n        year_begin (int): year in XXXX format for which year you want to start from \n        year_end (int): year in XXXX format for which year you want to end from \n        month (int): integer value of the month you are trying to query \n    returns: \n        df (pandas.DataFrame): a dataframe with these selected values \n    \"\"\"\n    \n    # connect database \n    conn = sqlite3.connect(db_file) \n    \n    # perform query\n    query = f\"\"\"\n    SELECT \n        temperatures.Year, \n        temperatures.Month, \n        temperatures.Temp, \n        stations.NAME, \n        stations.LATITUDE, \n        stations.LONGITUDE, \n        countries.Name \n    FROM \n        temperatures\n    JOIN \n        stations ON temperatures.ID = stations.ID\n    JOIN \n        countries ON substr(temperatures.ID, 1, 2) = countries.[FIPS 10-4]\n    WHERE \n        countries.Name = '{country}'\n        AND temperatures.Year BETWEEN {year_begin} AND {year_end}\n        AND temperatures.Month = {month}\n    \"\"\"\n    # convert query into dataframe \n    df = pd.read_sql_query(query, conn)\n    df = df.reindex(columns=['NAME', 'LATITUDE', 'LONGITUDE', 'Name', 'Year', 'Month', 'Temp'])\n    df.columns = ['NAME', 'LATITUDE', 'LONGITUDE', 'Country', 'Year', 'Month', 'Temp']\n    \n    # close connection \n    conn.close()\n    \n    return df \n\n\n\nSo this function takes 5 arguments 1. Database name 2. Country name 3. Beginning year 4. Ending year 5. Month number\nLet’s test it out with the code below:\n\nquery_climate_database(db_file = \"database.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns\n\n\n\n\n\nPart 3: Geographic scatterplots\nNow, let’s use SQL querying to create some geographic scatterplots with the NOAA data. First, let’s take a look at the function which will plot this figure for us:\n\nfrom climate_database import temperature_coefficient_plot\nprint(inspect.getsource(temperature_coefficient_plot))\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n     Function which creates a geographic scatterplot to see how yearly temperature increase vary across different locations (stations) for a given timeframe for a certain country. \n    \n    args: \n        db_file (str): string for database name \n        country (str): string for country to be plotted \n        year_begin (int): year in XXXX format for which year you want to start from \n        year_end (int): year in XXXX format for which year you want to end from\n        month (int): integer value of the month you are trying to query\n        min_obs (int): the minimum observations per data point plotted \n        **kwargs: arbitrary keyword arguments for the plot \n        \n    returns: \n        fig (plotly.express.scatter_mapbox): A geographic scatterplot with yearly temperature increase at different stations.\n    \"\"\"\n    # query climate database\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    # use transform function here to filter out the stations that don't have at least min_obs years worth of data \n    \n    # calculate yearly change in temperature\n    df = df[df.groupby('NAME')['Year'].transform('count') &gt;= min_obs] \n    \n    coefs = df.groupby('NAME').apply(coef).reset_index()\n    coefs.columns = ['NAME', 'YearlyChange']\n    \n    df = pd.merge(df, coefs, on='NAME')\n    \n    #df['YearlyChange'] = df['YearlyChange'].round(4)\n    \n    # create the scatterplot\n    fig = px.scatter_mapbox(df, lat='LATITUDE', lon='LONGITUDE', color='YearlyChange',\n                            color_continuous_midpoint=0, # centers the colorbar at 0 \n                            hover_data={'NAME': True, 'YearlyChange': ':.3f'}, # shows 3 digits after the decimal point \n                            labels={'YearlyChange': 'Estimated Yearly Increase (°C)'}, \n                            title=f\"Estimates of yearly increase in temperature in {pd.to_datetime(month, format='%m').month_name()} for stations in {country}, years {year_begin}-{year_end}\",  \n                            **kwargs)\n    \n    return fig\n\n\n\nAs we can see, the temperature_coefficient_plot function takes in 6 or more arguments: 1. database name 2. country name 3. beginning year 4. end year 5. month number 6. minimum observations in data point 7. additional keyword arguments for graph\nIt uses the first 5 arguments as input in the query_climate_database we tested earlier. Using this information, let’s create a geographic scatterplot of the yearly increase in temperature in January for India from 1980 to 2020.\n\nimport plotly.express as px\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"database.db\", \"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nInteresting! The estimated yearly increase actually appears to be decreasing near the Northeast region next to the Himalayas. I guess that makes sense, seeing how cold those mountains are. Let’s use the same function to see how this compares to Russia.\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"database.db\", \"Russia\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nCool! Russia’s temperatures appear to be decreasing in the same time period in several places as well.\n\n\nPart 4: Other types of Interactive Plots\nLet’s create a new query function that does not depend on the month. We can use this to create other kinds of interactive plots with plotly, that will answer two important questions:\nQuestion 1: How does the average yearly change in temperature vary based on elevation when comparing 2 countries in one timeframe?\nQuestion 2: How does the month impact the yearly increase in temperature for certain years in a given country?\nLet’s get started. First, let’s take a look at what the new query function looks like:\n\nfrom climate_database import query_climate_database_2\nprint(inspect.getsource(query_climate_database_2))\n\ndef query_climate_database_2(db_file, country, year_begin, year_end): \n    \"\"\"\n    Function which performs SQL query based on country, year beginning, and year end. \n    \n    args: \n        db_file (str): string for database name \n        country (str): string for country \n        year_begin (int): year in XXXX format for which year you want to start from \n        year_end (int): year in XXXX format for which year you want to end from \n    returns: \n        df (pandas.DataFrame): a dataframe with these selected values \n    \"\"\"\n    \n    # connect database \n    conn = sqlite3.connect(db_file) \n    \n    # perform sql query \n    query = f\"\"\"\n    SELECT \n        temperatures.Year, \n        temperatures.Month,  \n        temperatures.Temp, \n        stations.NAME, \n        stations.LATITUDE, \n        stations.LONGITUDE, \n        stations.STNELEV,\n        countries.Name\n    FROM \n        temperatures \n    JOIN \n        stations ON temperatures.ID = stations.ID\n    JOIN \n        countries ON substr(temperatures.ID, 1, 2) = countries.[FIPS 10-4]\n    WHERE \n        countries.Name = '{country}'\n        AND temperatures.Year BETWEEN {year_begin} AND {year_end}\n    \"\"\"\n    \n    # convert query into dataframe \n    df = pd.read_sql_query(query, conn) \n    \n    # close connection \n    conn.close() \n    return df\n\n\n\nAs we can see, this query function is the same as the previous one except it does not take in month. We can use this query function to create pandas dataframes that are more generalized, and then create plots to let us see more longterm trends. Let’s start with question 1, which is simple to plot but interesting to know.\n\nQuestion 1: How does the average yearly change in temperature vary based on elevation when comparing 2 countries in a given timeframe?\nTo answer this question, we will use the elevation_coefficient_plot() function. We will create a multifaceted, interactive plot. Let’s inspect this function to see what it looks like:\n\nfrom climate_database import elevation_coefficient_plot\nprint(inspect.getsource(elevation_coefficient_plot))\n\ndef elevation_coefficient_plot(db_file, country1, country2, year_begin, year_end, **kwargs): \n    \"\"\"\n     Function which creates an interactive plot for elevation vs. yearly increase in temperature for a given timeframe for two countries. \n    \n    args: \n        db_file (str): string for database name \n        country (str): string for country to be plotted \n        year_begin (int): year in XXXX format for which year you want to start from \n        year_end (int): year in XXXX format for which year you want to end from\n        **kwargs: arbitrary keyword arguments for the plot \n        \n    returns: \n        fig (plotly.express.scatter): A scatter plot of elevation on the x-variable and yearly change on the y-variable.  \n    \"\"\"\n    \n    # obtain one dataframe for each country \n    df1 = query_climate_database_2(db_file, country1, year_begin, year_end)\n    df2 = query_climate_database_2(db_file, country2, year_begin, year_end)\n    \n    # calculate coefs for each country \n    coefs1 = df1.groupby('NAME').apply(coef).reset_index()\n    coefs2 = df2.groupby('NAME').apply(coef).reset_index()\n    coefs1.columns = ['NAME','YearlyChange']\n    coefs1['Country'] = country1\n    coefs2.columns = ['NAME', 'YearlyChange']\n    coefs2['Country'] = country2\n    \n    # combine all findings \n    coefs_combined = pd.concat([coefs1, coefs2])\n    df1 = pd.merge(df1, coefs1, on='NAME') \n    df2 = pd.merge(df2, coefs2, on='NAME')\n    df_combined = pd.concat([df1, df2])\n    \n    # plot multifaceted figure \n    fig = px.scatter(df_combined, x='STNELEV',\n                     y='YearlyChange',\n                     hover_data={'NAME': True, 'LATITUDE': ':.3f', 'LONGITUDE': ':.3f', 'YearlyChange': ':.3f'},\n                     labels={'STNELEV': 'Station Elevation (meters)', 'YearlyChange': 'Yearly Temperature Change (°C)'},\n                     title=f'Correlation between Elevation and Yearly Increase in Temperature in {country1} and {country2} for the years {year_begin}-{year_end}', \n                     facet_col='Country',\n                     **kwargs)\n    return fig \n\n\n\nAs we can see, this function takes in the same arguments as query_climate_database_2, except it takes in two countries instead of 1, and also provides room for one more optional argument for additional keyword arguments. The way this function works is: 1. It first performs the query_climate_database_2 function to obtain the necessary data for country 1. 2. Repeat for country 2. 2. Then, it creates a combined dataframe for this data. 3. Finally, creates an interactive plotly scatterplot that allows us to see the correlation between elevation and average yearly change in temperature for these two countries.\nNow, let’s compare India vs China from 1990 to 2000.\n\nfig = elevation_coefficient_plot(\"database.db\", \"India\", \"China\", 1990, 2000)\nfig.show()\n\n\n\n\nWow! As we can see, each value is nicely rounded, and we can hover over each datapoint to learn more about it, such as the station name, the exact yearly temperature change, and more. However, there does not appear to be much correlation at all. So, to answer our question:\nIn both India and China, from 1990-2000, the average yearly change in temperature does not depend on elevation.\nHowever, we can see that China has more stations total, and these statiosn have higher elevations. This is one of the advantages of using multifaceted plots.\nWe can always make more scatterplots to see if there is any correlation in other countries!\nNow, however, we want to answer the question:\n\n\nQuestion 2: How does the month impact the yearly increase in temperature for certain years in a given country?\nTo do this, we will use the same query_climate_database_2 function, but this time, use the temperature_increase_boxplot function to graph it. Let’s, once again, take a look at what this function looks like.\n\nfrom climate_database import temperature_increase_boxplot\nprint(inspect.getsource(temperature_increase_boxplot))\n\ndef temperature_increase_boxplot(db_file, country, year_begin, year_end, **kwargs): \n    \"\"\"\n     Function which creates an interactive boxplot to see how yearly temperature increase varies across the months, for a given timeframe for a certain country. \n    \n    args: \n        db_file (str): string for database name \n        country (str): string for country to be plotted \n        year_begin (int): year in XXXX format for which year you want to start from \n        year_end (int): year in XXXX format for which year you want to end from\n        **kwargs: arbitrary keyword arguments for the plot \n        \n    returns: \n        fig (plotly.express.box): A boxplot of yearly temperature increase over the months.\n    \"\"\"\n    # perform sql query \n    df = query_climate_database_2(db_file, country, year_begin, year_end)\n\n    # find coefs for dataframe \n    coefs = df.groupby(['NAME', 'Month']).apply(coef).reset_index()\n    coefs.columns = ['NAME', 'Month', 'YearlyChange']\n    \n    # clean dataframe \n    df = pd.merge(df, coefs, on=['NAME', 'Month'])\n    df['Month'] = pd.to_datetime(df['Month'], format='%m').dt.month_name()\n    df['YearlyChange'] = df['YearlyChange'].round(3)\n    \n    # Create the boxplot\n    fig = px.box(df, x='Month', y='YearlyChange',\n                 color = 'Month',\n                 labels={'Month': 'Month', 'YearlyChange': 'Yearly Temperature Change (°C)'},\n                 title=f'Month-by-Month Statistics of Yearly Increase in Temperature in {country} from {year_begin}-{year_end}',\n                 hover_data={'YearlyChange': ':.3f'},\n                 **kwargs)\n    \n    return fig\n\n\n\ntemperature_increase_boxplot takes in the same arguments as elevation_coefficient_plot, except this time, it makes a box plot based on every month of the year. Let’s see this in action.\n\nfig = temperature_increase_boxplot(\"database.db\", \"India\", 1980, 2020)\n\nfig.show()\n\n\n\n\nWow, what an informative plot! The axes are nicely labelled, and, if we look closely, we can see a monthly pattern in yearly increase in temperature. It appears to be going in a slight wave up and down. However, the data points do have a lot of outliers.\n\n\n\nPart 5: Summary and takeaways\nToday, we learned how to use SQL querying to create pandas dataframes and create interactive plotly plots. We answered specific questions such as how to compare 2 different countries’ climate trends, and also how to view long-term yearly trends. SQL can open up many doors because we are not limited by our computer’s RAM. I hope you learned something from this post, and have a fantastic day!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Optimizing 2D heat diffusion by experimenting with matrix-vector multiplication, JAX, Numpy, and more.\n\n\n\n\n\n\nweek 7\n\n\nhomework4\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nAnika Misra\n\n\n\n\n\n\n\n\n\n\n\n\nHow to create a simple (pink) Message Bank Webapp that submits and views messages using Python and Flask\n\n\n\n\n\n\nweek 6\n\n\nhomework3\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nAnika Misra\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use Python and Scrapy to determine movies you will like based on Legally Blonde\n\n\n\n\n\n\nweek 4\n\n\nhomework2\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nAnika Misra\n\n\n\n\n\n\n\n\n\n\n\n\nUsing SQL Querying to compare different countries’ climate trends and also to compute year-long trends of temperature increases\n\n\n\n\n\n\nweek 3\n\n\nhomework1\n\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\nAnika Misra\n\n\n\n\n\n\n\n\n\n\n\n\nHW0\n\n\n\n\n\n\nweek 2\n\n\nhomework0\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\nAnika Misra\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nart\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nAnika Misra\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nAnika Misra\n\n\n\n\n\n\nNo matching items"
  }
]