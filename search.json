[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/hw-0/index.html",
    "href": "posts/hw-0/index.html",
    "title": "HW0",
    "section": "",
    "text": "In today’s dataset from Github, I will be explaining how to create a simple visualization from the Palmer Penguins dataset. There are many features in this dataset, such as species, region, island, egg stage, culmen features, body mass, gender, and delta 15 N features. However, the question we are trying to answer today is:\n\n\nLet’s get started."
  },
  {
    "objectID": "posts/hw-0/index.html#improvements-and-next-steps",
    "href": "posts/hw-0/index.html#improvements-and-next-steps",
    "title": "HW0",
    "section": "Improvements and next steps:",
    "text": "Improvements and next steps:\n\ncreate DataFrames of each island and species to determine the exact range of body masses\nGet more specific statistics and exact numbers as opposed to relying on visualizations\nGet more data from Chinstrap and Gentoo penguins that comes from other islands\n\nI hope you learned something with today’s blog post! Have an ice day.\n\n\n\nHave an ice day.png\n\n\nImage sources: Ice\nPenguin"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in my Quarto blog. Just testing things out. Here is a painting I made two days ago."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This python code is known as one of the most complex pieces of code ever written. If you run this in your IDE, it might blow your mind:\n\nprint(\"hello world\") \n\nhello world"
  },
  {
    "objectID": "posts/hw-2/index.html",
    "href": "posts/hw-2/index.html",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "",
    "text": "Welcome! In today’s post, I will be describing how to build a webscraper to scrape the TMDB movie database. By the end of this post, you should understand how to build a webscraper, and you should be able to use this webscraper for the movie of your choice to find recommendations based on it. For my movie, I’m going to choose Legally Blonde.\n\n\n\nSegmentLocal\n\n\nImage source\nWhat a girlboss!!"
  },
  {
    "objectID": "posts/hw-2/index.html#part-0a-initializer",
    "href": "posts/hw-2/index.html#part-0a-initializer",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 0A: Initializer",
    "text": "Part 0A: Initializer\nJust like Elle Woods had to study hard for her LSAT before she got into Harvard, we all have to start somewhere. Before webscraping the TMDB website, we have to build our scraper. In this section, I will explain how to build the scraper and how the scraper works. Then, in part B, I will explain where this TmdbSpider class should go and how to actually set up the scraper in your local computer. So for now, just follow along with the explanation, and we will set everything up in Part B.\nWe begin this webscraper by importing the necessary packages, defining a class, and constructing the initializer.\nWe will name the class TmdbSpider since we are creating a webscraper for the TMDB database. It will inherit the class scrapy.Spider.\nIn the initializer, we will accept a “subdir” string argument. This argument is a string that contains the last part of the url the movie we want to scrape. It should look something like this:\n“8835-legally-blonde”\nThis url ending can be found by looking at the url of the movie we want to scrape on the TMDB website.\nThen, we will create an instance variable called start_urls that builds the complete url for the movie to scrape.\n\n# import necessary packages \nimport scrapy \n\n# define a class that inherits scrapy.Spider, the base class for spiders \nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider' # name our spider \n    def __init__(self, subdir=None, *args, **kwargs):\n        \"\"\" \n        Class constructor for the TmdbSpider class. \n        \n        Args: \n            self (TmdbSpider class instance): instance of the TmdbSpider class. \n            subdir (string): String for the subdirectory of the movie we want to start with. This string can be found in the movie url \n            *args: additional arbitrary keyword arguments \n            **kwargs: additional keyword arguments  \n            \n        \"\"\" \n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"] # build the complete url from the subdirectory"
  },
  {
    "objectID": "posts/hw-2/index.html#part-1a-parse-method",
    "href": "posts/hw-2/index.html#part-1a-parse-method",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 1A: Parse Method",
    "text": "Part 1A: Parse Method\nThe first class method we will define is the parse() function. This is how the parse() function works:\n\nFirst, obtain the url of the full cast and crew. This will be done by “hardcoding”, aka, manually adding a “cast” string onto our start_urls instance variable.\nCall the parse_full_credits() method. This will be done by specifying a callback argument to a yielded scrapy request.\n\nThis method assumes we start on a movie page, and it should navigate us to a “full cast and crew” page for that specific movie. Then, it calls the next parse function, which we will define in part 2.\n\ndef parse(self, response): \n        \"\"\"\n        Assumptions: \n            Assume we are starting on a movie page. \n        Effects: \n            Navigate to a \"full cast and crew\" page for that specific movie. \n            Data outputs: A yielded scrapy request calling parse_full_credits() method with the harcoded \"full cast and crew\" url \n        \"\"\"\n        \n        # first, obtain the url of full cast and crew (hardcoded)\n        # response.url is a built-in method that gives us current url of response\n        cast_url = response.url + '/cast/'  \n          \n        \n        # parse_full_credits method is called by specifying callback argument to yielded scrapy request\n        yield scrapy.Request(url = cast_url, callback = self.parse_full_credits)"
  },
  {
    "objectID": "posts/hw-2/index.html#important",
    "href": "posts/hw-2/index.html#important",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Important:",
    "text": "Important:\nPlease note that each of the parse methods we are defining are all part of the TmdbSpider class!!!\n\nAnyways…\nWhat just happened in the last line? Well, in the “yield scrapy.Request” line, we first tell scrapy to fetch this url. After it fetches the “full cast and crew” url, we obtain the response object. This response object is automatically passed in as the second argument to the callback function, parse_full_credits. And, remember from PIC16A that “self” is always passed in as the first argument. This is why parse_full_credits function has no arguments specified.\nparse_full_credits: 1. First argument: self (automatic) 2. Second argument: response object from cast_url (automatic)\nWhat does the parse_full_credits() method do? Well, let’s define it now!"
  },
  {
    "objectID": "posts/hw-2/index.html#part-2a-parse-full-credits-method",
    "href": "posts/hw-2/index.html#part-2a-parse-full-credits-method",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 2A: Parse full credits Method",
    "text": "Part 2A: Parse full credits Method\nThe second class method we will define is parse_full_credits(). This function takes in two arguments: a reference to the current instance of the class “self”, and the response object generated from the previous parse method. It assumes we start on the “full cast and crew” wegpage for a specific movie. Here is how it works:\n\nLocate the “cast” table of the webpage.\n\n\nTo do this, we will search for an h3 element containing “Cast” using Xpath.\nWe must ensure we are only in the cast table, or else we might get non-actors in our list (aka, members from the production crew).\n\n\nObtain the list of cast members.\n\n\nSelect the first “ordered list” (ol) element that appears after the h3 element. We can do this using “following-sibling::ol[1]”.\nNote that the ordered list element is not a “child” of the h3 element, which is why we use the “following-sibling” expression.\n\n\nObtain the actor urls.\n\n\nUsing the xpath object of cast members, navigate into the list element that contains a div element with a class attribute containing “info”.\nThen, look for a &lt;&gt; tag after the div class. This &lt;&gt; tag may or may not be the direct descendent of the div element, which is why we use the “descendant-or-self” line.\nFinally, navigate into the &lt;&gt; tag and grab the url inside it. Use the “getall()” method to obtain all such links.\n\n\nYield scrapy requests for each actor url, using the parse_actor_page method as a callback function.\n\nHere is what the function should look like.\n\ndef parse_full_credits(self, response): \n        \"\"\"\n        Assumptions: \n            Assume we are starting on a \"full cast and crew\" page for one movie. \n        Effects: \n            Navigate to each actor's page for the specific movie (note - ACTOR - not any other crew member!) \n            Data outputs: A yielded scrapy request calling parse_actor_page() method for every single actor's link in this webpage. \n        \"\"\"\n        \n        # Step 1: Locate cast table of webpage (h3 tag contains \"Cast\")  \n        # Step 2: Create an xpath object from this \"Cast\" table, called table1 \n        table1 = response.xpath('//h3[contains(., \"Cast\")]/following-sibling::ol[1]')\n        \n        # Step 3: Obtain all the actor urls by finding the proper link for each actor  \n        urls_actors = table1.xpath('.//li//div[contains(@class, \"info\")]/descendant-or-self::*/p/a/@href').getall()\n        # notice that we use getall() method to obtain ALL actor urls \n        \n        # Step 4: Yield scrapy requests for each actor url \n        for link in urls_actors: \n            yield scrapy.Request(url = response.urljoin(link), callback = self.parse_actor_page) \n            # we use urljoin as opposed to simple string concatonation to ensure that relative urls are formed properly \n\nHere is what the HTML element looks like. In this HTML element, we are trying to obtain the second “a href=”/person/368-reese-witherspoon” text to get the link for the actor page. We specifically try to get the second one, and not the first one, because each actor has two links and we do not want more than one link per actor.\n\n\n\nScreenshot 2024-02-05 at 1.31.27 PM.png\n\n\nIn our function, just like before, we do not put any arguments into the parse_actor_page() function because the second argument is automatically the response object fetched by the url we specified, and the first argument is automatically “self”. But what is the parse_actor_page() function? Let’s define it right now!"
  },
  {
    "objectID": "posts/hw-2/index.html#part-3a-parse-actor-page",
    "href": "posts/hw-2/index.html#part-3a-parse-actor-page",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 3A: Parse actor page",
    "text": "Part 3A: Parse actor page\nThe final class method we will define is the parse_actor_page() method. This function takes in two arguments, (1) a reference to the current instance of the class “self”, and (2) the response object generated from the previous class method we defined. It assumes we start on an actor’s webpage. It will yield a dictionary object for each actor and each movie they played in. Here is how it works:\n\nGet the actor’s name: It will be on the very top of the webpage in the &lt;&gt; title element.\nLocate the “acting” table in the actor’s webpage and create an xpath object from it called “table”.\n\n\nWe do this by first finding the h3 element with text element “Acting”, and then we select the element that occurs directly after it.\nThis is why we use the “following-sibling” code.\n\n\nObtain each movie / tv show name from the acting “table”.\n\n\nThis is done by navigating to the &lt;&gt; element within the acting “table”.\n\n\nYield a key-pair value containing {actor_name, movie_or_TV_name} for every movie / tv show that the actor played in.\n\n\ndef parse_actor_page(self, response): \n        \"\"\"\n        Assumptions: \n            Assume we are starting on an actor's webpage. \n        Effects: \n            Does not navigate to any new webpages.  \n            Data outputs: A yielded dictionary, each key-value pair containing the movie that an actor acted in along with the actor's name. \n        \"\"\"\n        \n        # Step 1: obtain the actor's name from the Title of the webpage \n        actor_name = response.css('h2.title &gt; a::text').get()\n        \n        # Step 2: Locate the acting table in the webpage, and select the element that occurs immediately after it \n        table = response.xpath('//h3[text()=\"Acting\"]/following-sibling::table[1]')\n        \n        # Step 3: Obtain the relevant movie / tv show names from each acting \"table\"  \n        movie_names = table.css('td.role.true.account_adult_false.item_adult_false &gt; a.tooltip &gt; bdi::text').getall()\n        # notice that we use \"getall()\" method to obtain ALL movie / tv show names  \n        \n        # Yield a key-value pair for each movie / tv show in this actor's webpage \n        for movie_or_TV_name in movie_names: \n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name} \n\nHere is what the HTML element looks like. In this HTML element, we are trying to obtain the text within the &lt;&gt; element because that gives us the movie / tv show name.\n\n\n\nScreenshot 2024-02-05 at 5.19.29 PM.png"
  },
  {
    "objectID": "posts/hw-2/index.html#part-0b-setting-up-scraper",
    "href": "posts/hw-2/index.html#part-0b-setting-up-scraper",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 0B: Setting up scraper",
    "text": "Part 0B: Setting up scraper\nNow that we know how to build our TMDB Spider, let’s actually set it up and use it to find recommendations based on Legally Blonde. The first step is to set up your local machine for webscraping. Here are the first few steps:\nIn your terminal, 1. Activate the Python environment of your choice 2. Navigate into the directory where you want your scraper files to be\n3. Run the following lines in your terminal:\nscrapy startproject TMDB_scraper cd TMDB_scraper\nThis will create a lot of folders and files. Do not worry about these. Here are the next steps: 1. Inside your spider directory, add a file called “tmdb_spider.py”. Inside this file, write out the tmdbSpider class we just defined. 2. In your settings.py file, add this line: “USER_AGENT = ‘Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0’”. This will ensure we do not run into any 403 errors."
  },
  {
    "objectID": "posts/hw-2/index.html#part-1b-running-the-scraper",
    "href": "posts/hw-2/index.html#part-1b-running-the-scraper",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 1B: Running the scraper",
    "text": "Part 1B: Running the scraper\nNow, we will run our scraper for Legally Blonde. The TMDB link for Legally Blonde is:\nhttps://www.themoviedb.org/movie/8835-legally-blonde\nSo, our “subdir” argument should be 8835-legally-blonde. Navigate into the directory where you want your results file to be, and run the following line in your terminal:\nscrapy crawl tmdb_spider -o results.csv -a subdir=8835-legally-blonde\nThis line will webscrape the Legally Blonde database using the tmdb spider class we just defined. The “subdir” argument is passed into the very first “parse” method, and the webscraper navigates to the appropriate webpages from there."
  },
  {
    "objectID": "posts/hw-2/index.html#part-2b-accessing-and-analyzing-results.csv-file",
    "href": "posts/hw-2/index.html#part-2b-accessing-and-analyzing-results.csv-file",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 2B: Accessing and analyzing results.csv file",
    "text": "Part 2B: Accessing and analyzing results.csv file\nAfter running this line in your terminal, wait till you see a message that the Spider has closed in your terminal. Then, in the folder that you are currently in, there should be a file called “results.csv”. This should contain two columns: 1. Actor name 2. Movie name\nNow, let’s analyze these results to build a recommender system.\nFirst, let’s import this results.csv file and turn it into a pandas dataframe for easy analysis.\n\n# import pandas \nimport pandas as pd\n\n# import results csv file as dataframe \nresults = pd.read_csv(\"results.csv\") \n\n# preview results dataframe\nresults.head()\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nReese Witherspoon\nTiny Trailblazers\n\n\n1\nReese Witherspoon\nTracy Flick Can't Win\n\n\n2\nReese Witherspoon\nGreat Performers: 9 Kisses\n\n\n3\nReese Witherspoon\nYou’re Cordially Invited\n\n\n4\nReese Witherspoon\nLegally Blonde 3\n\n\n\n\n\n\n\nGreat! Let’s see how many actors were in the original “Legally Blonde” movie, and how many total actors and movie / TV show names there are.\n\nactors_in_legally_blonde = results[results['movie_or_TV_name'] == 'Legally Blonde']['actor'].unique()\nmovies_total = results['movie_or_TV_name'].unique()\n\nprint(\"All the actors in Legally Blonde: \\n\", actors_in_legally_blonde, \"\\n\")\n\nprint(\"All the movies that are in the results file: \\n\", movies_total, \"\\n\")\nprint(\"Number of movies / TV shows in the results file: \\n\", results['movie_or_TV_name'].nunique())\n\nAll the actors in Legally Blonde: \n ['Reese Witherspoon' 'Lily' 'Kelly Driscoll' 'Sasha Barrese' 'Moonie'\n 'Kennedy Stone' 'Elizabeth Matthews' 'Richard Hillman' 'John Kapelos'\n 'Patricia Kimes' 'Jodi Harris' 'Nectar Rose' 'Terence Michael'\n 'John Cantwell' 'Ondrea de Vincentis' 'Chaney Kley' 'Melissa Anne Young'\n 'Brody Hutzler' 'Lacey Beeman' 'Jason Christopher' 'Lisa K. Wyatt'\n 'Corinne Reilly' 'Doug Spinuzza' 'Niklaus Lange' 'Victoria Mahoney'\n 'Tane McClure' 'David Moreland' 'Allyce Beasley' 'Kevin Cooney'\n 'Cici Lau' 'Natalie Barish' 'Lisa Arch' 'Francesca P. Roberts'\n 'Kimberly McCullough' \"Shannon O'Hurley\" 'Ted Rooney' 'Kelly Nyks'\n 'Samantha Lemole' 'Michael B. Silver' 'Ted Kairys' 'Bruce Thomas'\n 'Meredith Scott Lynn' 'Linda Cardellini' 'Raquel Welch' 'Greg Serano'\n 'Osgood Perkins' 'Wayne Federman' 'Jessica Cauffiel' 'Alanna Ubach'\n 'Victor Garber' 'James Read' 'Jennifer Coolidge' 'Holland Taylor'\n 'Matthew Davis' 'Selma Blair' 'Luke Wilson' 'Ali Larter'] \n\nAll the movies that are in the results file: \n ['Tiny Trailblazers' \"Tracy Flick Can't Win\" 'Great Performers: 9 Kisses'\n ... 'Giving It Up' 'Varsity Blues' 'Just Shoot Me!'] \n\nNumber of movies / TV shows in the results file: \n 1775"
  },
  {
    "objectID": "posts/hw-2/index.html#part-3b-building-our-recommender-system",
    "href": "posts/hw-2/index.html#part-3b-building-our-recommender-system",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 3B: Building our recommender system",
    "text": "Part 3B: Building our recommender system\nThere are 1775 movies/TV shows to choose from! How do we know which one we will like best? One way to do this is to see which movies / TV shows have the highest number of shared actors with Legally Blonde. Then, we can create a visualization with the top 10 movies / TV shows and the number of shared actors they contain.\nLet’s make this visualization pink, Elle Woods’ favorite color!!\n\n\n\nImage 2-5-24 at 6.40 PM.jpg\n\n\nImage source\n\n# import matplotlib \nimport matplotlib.pyplot as plt \n\n# create a new dataframe for shared actors \nsame_actors = results[results['actor'].isin(actors_in_legally_blonde) & (results['movie_or_TV_name'] != 'Legally Blonde')] \n\n# find out how many shared actors per movie \nsame_actors_value = same_actors['movie_or_TV_name'].value_counts()\n\n# create a new dataframe with top 10 movies / TV shows \ntop_movie_TV_show = same_actors_value.nlargest(10)\n\n# plot bar chart accordingly \nplt.figure(figsize=(10, 6))\n# make sure the color is pink! \ntop_movie_TV_show.plot(kind='bar', color='pink')\nplt.title('Most recommended movies / TV shows for you based on Legally Blonde')\nplt.xlabel('Movie / TV Show Name')\nplt.ylabel('Number of Shared Actors')\nplt.show()"
  },
  {
    "objectID": "posts/hw-2/index.html#part-4b-summary-and-takeaways",
    "href": "posts/hw-2/index.html#part-4b-summary-and-takeaways",
    "title": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde",
    "section": "Part 4B: Summary and takeaways",
    "text": "Part 4B: Summary and takeaways\nWow! Now we have a visualization for a recommender system based on Legally Blonde. I am shocked there are so many true crime shows on there! I guess Reese Witherspoon really stuck to the “Legal” movie genre. Good for her! She’s such a slay girlboss queen.\nSomething else that is pretty cool about this visualization is that it not only shows you which movies and TV shows are recommended for you, but at which level they are recommended for you based on the number of actors.\nNote that you can do this for any movie of your choice using the scraper we built!\n\nThanks for reading!! And remember: Whoever said orange is the new pink was seriously disturbed."
  },
  {
    "objectID": "posts/hw-1/index.html",
    "href": "posts/hw-1/index.html",
    "title": "Using SQL Querying to compare different countries’ climate trends and also to compute year-long trends of temperature increases",
    "section": "",
    "text": "Welcome!\nToday we will be creating several interactive plots from the NOAA climate dataset. However, because this dataset is extremely large, we will be taking advantage of SQL querying, which allows us to select the data we need without uploading the entire file and using up too much memory.\n\n\nSection 1: Creating the database\nFirst, we will upload all necessary packages.\n\nimport sqlite3\nimport pandas as pd\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nNow let’s enter the relevant file names.\n\ncountries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ntemps_url = \"temps.csv\"\nstation_url = \"station-metadata.csv\"\n\n# add link to download other tables \n\nNow we will create a database called my_database.db.\n\nconn = sqlite3.connect(\"database.db\") # this creates a database in current directory called my_database.db\n\nNow we are ready to add our first table to our database. The entire purpose of using SQL querying is that we avoid uploading the entire table in order to prevent overusage of RAM. Because the temperatures dataset is extremely large, for this reason, we will upload it chunk by chunk. And, because the other datasets are smaller, we can just upload them all at once.\nLet’s set the chunksize of the temps dataframe to 100,000. With the chunksize keyword specified, the read_csv function of pandas creates an iterator of the temps dataframe.\n\nCHUNKSIZE=100000\ntemps_df_iter = pd.read_csv(temps_url, chunksize=CHUNKSIZE)\ntemps_df = temps_df_iter.__next__()\ntemps_df.shape\n\n(100000, 14)\n\n\nAs we can see, the temps dataframe indeed has only 100,000 rows! Let’s finish uploading the other files and then preview everything.\n\ncountries_df = pd.read_csv(countries_url)\ncountries_df.shape\n\n(279, 3)\n\n\n\nstations_df = pd.read_csv(station_url) \nstations_df.shape\n\n(27585, 5)\n\n\nAs we can see, the stations and countries files are not nearly as large as the temperatures file. Now, let’s preview what each table looks like so we are familiar with the column names and how to query accordingly.\n\ntemps_df.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\n\ncountries_df.head()\n\n\n\n\n\n\n\n\nFIPS 10-4\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa\n\n\n\n\n\n\n\n\nstations_df.head()\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\n\n\n\n\n\n\n\nGreat! We are finally ready to add tables to our database. We will add three tables, one for temperatures, one for stations, and one for countries.\nFirst, let’s create a function to clean up the temperatures dataframe a bit.\n\ndef prepare_df(df):\n    \"\"\"\n    Function that cleans and reorganizes the temperatures dataframe. \n    \n    args: \n        df (pandas.DataFrame): the dataframe to organize, from the temperatures file of the NOAA database \n    returns: \n        df (pandas.DataFrame): the reorganized dataframe \n    \"\"\"\n    \n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\nNow let’s add the temperatures dataframe to our SQL database as a table by working with only 100,000 rows at a time. We are able to do this thanks to the iterator we created.\n\nfor i, df in enumerate(temps_df_iter):\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\nNow, let’s add the other two dataframes as tables to our database.\n\nstations_df.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\ncountries_df.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n279\n\n\nLet’s make sure that everything was added to our database successfully. Running the following code will allow us to see what is currently in our database.\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\nAwesome! We have three tables, one for temps, one for stations, and one for countries. Now, we are done with our database connection, so it is good practice to close it.\n\nconn.close()\n\n\n\nPart 2: Querying our first database\nLet’s now use our database to perform an SQL query that returns a pandas dataframe. The beauty of this is that we do not have to have the entire dataframe in our RAM. To do this, we will use the query_climate_database() function which I have already defined. Let’s see what it looks like below:\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month): \n    \"\"\"\n    Function which performs SQL query based on country, year beginning, year end, and month. \n    \n    args: \n        db_file (str): string for database name \n        country (str): string for country \n        year_begin (int): year in XXXX format for which year you want to start from \n        year_end (int): year in XXXX format for which year you want to end from \n        month (int): integer value of the month you are trying to query \n    returns: \n        df (pandas.DataFrame): a dataframe with these selected values \n    \"\"\"\n    \n    # connect database \n    conn = sqlite3.connect(db_file) \n    \n    # perform query\n    query = f\"\"\"\n    SELECT \n        temperatures.Year, \n        temperatures.Month, \n        temperatures.Temp, \n        stations.NAME, \n        stations.LATITUDE, \n        stations.LONGITUDE, \n        countries.Name \n    FROM \n        temperatures\n    JOIN \n        stations ON temperatures.ID = stations.ID\n    JOIN \n        countries ON substr(temperatures.ID, 1, 2) = countries.[FIPS 10-4]\n    WHERE \n        countries.Name = '{country}'\n        AND temperatures.Year BETWEEN {year_begin} AND {year_end}\n        AND temperatures.Month = {month}\n    \"\"\"\n    # convert query into dataframe \n    df = pd.read_sql_query(query, conn)\n    df = df.reindex(columns=['NAME', 'LATITUDE', 'LONGITUDE', 'Name', 'Year', 'Month', 'Temp'])\n    df.columns = ['NAME', 'LATITUDE', 'LONGITUDE', 'Country', 'Year', 'Month', 'Temp']\n    \n    # close connection \n    conn.close()\n    \n    return df \n\n\n\nSo this function takes 5 arguments 1. Database name 2. Country name 3. Beginning year 4. Ending year 5. Month number\nLet’s test it out with the code below:\n\nquery_climate_database(db_file = \"database.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns\n\n\n\n\n\nPart 3: Geographic scatterplots\nNow, let’s use SQL querying to create some geographic scatterplots with the NOAA data. First, let’s take a look at the function which will plot this figure for us:\n\nfrom climate_database import temperature_coefficient_plot\nprint(inspect.getsource(temperature_coefficient_plot))\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n     Function which creates a geographic scatterplot to see how yearly temperature increase vary across different locations (stations) for a given timeframe for a certain country. \n    \n    args: \n        db_file (str): string for database name \n        country (str): string for country to be plotted \n        year_begin (int): year in XXXX format for which year you want to start from \n        year_end (int): year in XXXX format for which year you want to end from\n        month (int): integer value of the month you are trying to query\n        min_obs (int): the minimum observations per data point plotted \n        **kwargs: arbitrary keyword arguments for the plot \n        \n    returns: \n        fig (plotly.express.scatter_mapbox): A geographic scatterplot with yearly temperature increase at different stations.\n    \"\"\"\n    # query climate database\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    # use transform function here to filter out the stations that don't have at least min_obs years worth of data \n    \n    # calculate yearly change in temperature\n    df = df[df.groupby('NAME')['Year'].transform('count') &gt;= min_obs] \n    \n    coefs = df.groupby('NAME').apply(coef).reset_index()\n    coefs.columns = ['NAME', 'YearlyChange']\n    \n    df = pd.merge(df, coefs, on='NAME')\n    \n    #df['YearlyChange'] = df['YearlyChange'].round(4)\n    \n    # create the scatterplot\n    fig = px.scatter_mapbox(df, lat='LATITUDE', lon='LONGITUDE', color='YearlyChange',\n                            color_continuous_midpoint=0, # centers the colorbar at 0 \n                            hover_data={'NAME': True, 'YearlyChange': ':.3f'}, # shows 3 digits after the decimal point \n                            labels={'YearlyChange': 'Estimated Yearly Increase (°C)'}, \n                            title=f\"Estimates of yearly increase in temperature in {pd.to_datetime(month, format='%m').month_name()} for stations in {country}, years {year_begin}-{year_end}\",  \n                            **kwargs)\n    \n    return fig\n\n\n\nAs we can see, the temperature_coefficient_plot function takes in 6 or more arguments: 1. database name 2. country name 3. beginning year 4. end year 5. month number 6. minimum observations in data point 7. additional keyword arguments for graph\nIt uses the first 5 arguments as input in the query_climate_database we tested earlier. Using this information, let’s create a geographic scatterplot of the yearly increase in temperature in January for India from 1980 to 2020.\n\nimport plotly.express as px\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"database.db\", \"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nInteresting! The estimated yearly increase actually appears to be decreasing near the Northeast region next to the Himalayas. I guess that makes sense, seeing how cold those mountains are. Let’s use the same function to see how this compares to Russia.\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"database.db\", \"Russia\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nCool! Russia’s temperatures appear to be decreasing in the same time period in several places as well.\n\n\nPart 4: Other types of Interactive Plots\nLet’s create a new query function that does not depend on the month. We can use this to create other kinds of interactive plots with plotly, that will answer two important questions:\nQuestion 1: How does the average yearly change in temperature vary based on elevation when comparing 2 countries in one timeframe?\nQuestion 2: How does the month impact the yearly increase in temperature for certain years in a given country?\nLet’s get started. First, let’s take a look at what the new query function looks like:\n\nfrom climate_database import query_climate_database_2\nprint(inspect.getsource(query_climate_database_2))\n\ndef query_climate_database_2(db_file, country, year_begin, year_end): \n    \"\"\"\n    Function which performs SQL query based on country, year beginning, and year end. \n    \n    args: \n        db_file (str): string for database name \n        country (str): string for country \n        year_begin (int): year in XXXX format for which year you want to start from \n        year_end (int): year in XXXX format for which year you want to end from \n    returns: \n        df (pandas.DataFrame): a dataframe with these selected values \n    \"\"\"\n    \n    # connect database \n    conn = sqlite3.connect(db_file) \n    \n    # perform sql query \n    query = f\"\"\"\n    SELECT \n        temperatures.Year, \n        temperatures.Month,  \n        temperatures.Temp, \n        stations.NAME, \n        stations.LATITUDE, \n        stations.LONGITUDE, \n        stations.STNELEV,\n        countries.Name\n    FROM \n        temperatures \n    JOIN \n        stations ON temperatures.ID = stations.ID\n    JOIN \n        countries ON substr(temperatures.ID, 1, 2) = countries.[FIPS 10-4]\n    WHERE \n        countries.Name = '{country}'\n        AND temperatures.Year BETWEEN {year_begin} AND {year_end}\n    \"\"\"\n    \n    # convert query into dataframe \n    df = pd.read_sql_query(query, conn) \n    \n    # close connection \n    conn.close() \n    return df\n\n\n\nAs we can see, this query function is the same as the previous one except it does not take in month. We can use this query function to create pandas dataframes that are more generalized, and then create plots to let us see more longterm trends. Let’s start with question 1, which is simple to plot but interesting to know.\n\nQuestion 1: How does the average yearly change in temperature vary based on elevation when comparing 2 countries in a given timeframe?\nTo answer this question, we will use the elevation_coefficient_plot() function. We will create a multifaceted, interactive plot. Let’s inspect this function to see what it looks like:\n\nfrom climate_database import elevation_coefficient_plot\nprint(inspect.getsource(elevation_coefficient_plot))\n\ndef elevation_coefficient_plot(db_file, country1, country2, year_begin, year_end, **kwargs): \n    \"\"\"\n     Function which creates an interactive plot for elevation vs. yearly increase in temperature for a given timeframe for two countries. \n    \n    args: \n        db_file (str): string for database name \n        country (str): string for country to be plotted \n        year_begin (int): year in XXXX format for which year you want to start from \n        year_end (int): year in XXXX format for which year you want to end from\n        **kwargs: arbitrary keyword arguments for the plot \n        \n    returns: \n        fig (plotly.express.scatter): A scatter plot of elevation on the x-variable and yearly change on the y-variable.  \n    \"\"\"\n    \n    # obtain one dataframe for each country \n    df1 = query_climate_database_2(db_file, country1, year_begin, year_end)\n    df2 = query_climate_database_2(db_file, country2, year_begin, year_end)\n    \n    # calculate coefs for each country \n    coefs1 = df1.groupby('NAME').apply(coef).reset_index()\n    coefs2 = df2.groupby('NAME').apply(coef).reset_index()\n    coefs1.columns = ['NAME','YearlyChange']\n    coefs1['Country'] = country1\n    coefs2.columns = ['NAME', 'YearlyChange']\n    coefs2['Country'] = country2\n    \n    # combine all findings \n    coefs_combined = pd.concat([coefs1, coefs2])\n    df1 = pd.merge(df1, coefs1, on='NAME') \n    df2 = pd.merge(df2, coefs2, on='NAME')\n    df_combined = pd.concat([df1, df2])\n    \n    # plot multifaceted figure \n    fig = px.scatter(df_combined, x='STNELEV',\n                     y='YearlyChange',\n                     hover_data={'NAME': True, 'LATITUDE': ':.3f', 'LONGITUDE': ':.3f', 'YearlyChange': ':.3f'},\n                     labels={'STNELEV': 'Station Elevation (meters)', 'YearlyChange': 'Yearly Temperature Change (°C)'},\n                     title=f'Correlation between Elevation and Yearly Increase in Temperature in {country1} and {country2} for the years {year_begin}-{year_end}', \n                     facet_col='Country',\n                     **kwargs)\n    return fig \n\n\n\nAs we can see, this function takes in the same arguments as query_climate_database_2, except it takes in two countries instead of 1, and also provides room for one more optional argument for additional keyword arguments. The way this function works is: 1. It first performs the query_climate_database_2 function to obtain the necessary data for country 1. 2. Repeat for country 2. 2. Then, it creates a combined dataframe for this data. 3. Finally, creates an interactive plotly scatterplot that allows us to see the correlation between elevation and average yearly change in temperature for these two countries.\nNow, let’s compare India vs China from 1990 to 2000.\n\nfig = elevation_coefficient_plot(\"database.db\", \"India\", \"China\", 1990, 2000)\nfig.show()\n\n\n\n\nWow! As we can see, each value is nicely rounded, and we can hover over each datapoint to learn more about it, such as the station name, the exact yearly temperature change, and more. However, there does not appear to be much correlation at all. So, to answer our question:\nIn both India and China, from 1990-2000, the average yearly change in temperature does not depend on elevation.\nHowever, we can see that China has more stations total, and these statiosn have higher elevations. This is one of the advantages of using multifaceted plots.\nWe can always make more scatterplots to see if there is any correlation in other countries!\nNow, however, we want to answer the question:\n\n\nQuestion 2: How does the month impact the yearly increase in temperature for certain years in a given country?\nTo do this, we will use the same query_climate_database_2 function, but this time, use the temperature_increase_boxplot function to graph it. Let’s, once again, take a look at what this function looks like.\n\nfrom climate_database import temperature_increase_boxplot\nprint(inspect.getsource(temperature_increase_boxplot))\n\ndef temperature_increase_boxplot(db_file, country, year_begin, year_end, **kwargs): \n    \"\"\"\n     Function which creates an interactive boxplot to see how yearly temperature increase varies across the months, for a given timeframe for a certain country. \n    \n    args: \n        db_file (str): string for database name \n        country (str): string for country to be plotted \n        year_begin (int): year in XXXX format for which year you want to start from \n        year_end (int): year in XXXX format for which year you want to end from\n        **kwargs: arbitrary keyword arguments for the plot \n        \n    returns: \n        fig (plotly.express.box): A boxplot of yearly temperature increase over the months.\n    \"\"\"\n    # perform sql query \n    df = query_climate_database_2(db_file, country, year_begin, year_end)\n\n    # find coefs for dataframe \n    coefs = df.groupby(['NAME', 'Month']).apply(coef).reset_index()\n    coefs.columns = ['NAME', 'Month', 'YearlyChange']\n    \n    # clean dataframe \n    df = pd.merge(df, coefs, on=['NAME', 'Month'])\n    df['Month'] = pd.to_datetime(df['Month'], format='%m').dt.month_name()\n    df['YearlyChange'] = df['YearlyChange'].round(3)\n    \n    # Create the boxplot\n    fig = px.box(df, x='Month', y='YearlyChange',\n                 color = 'Month',\n                 labels={'Month': 'Month', 'YearlyChange': 'Yearly Temperature Change (°C)'},\n                 title=f'Month-by-Month Statistics of Yearly Increase in Temperature in {country} from {year_begin}-{year_end}',\n                 hover_data={'YearlyChange': ':.3f'},\n                 **kwargs)\n    \n    return fig\n\n\n\ntemperature_increase_boxplot takes in the same arguments as elevation_coefficient_plot, except this time, it makes a box plot based on every month of the year. Let’s see this in action.\n\nfig = temperature_increase_boxplot(\"database.db\", \"India\", 1980, 2020)\n\nfig.show()\n\n\n\n\nWow, what an informative plot! The axes are nicely labelled, and, if we look closely, we can see a monthly pattern in yearly increase in temperature. It appears to be going in a slight wave up and down. However, the data points do have a lot of outliers.\n\n\n\nPart 5: Summary and takeaways\nToday, we learned how to use SQL querying to create pandas dataframes and create interactive plotly plots. We answered specific questions such as how to compare 2 different countries’ climate trends, and also how to view long-term yearly trends. SQL can open up many doors because we are not limited by our computer’s RAM. I hope you learned something from this post, and have a fantastic day!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "How to use Python and Scrapy to determine movies you will like based on Legally Blonde\n\n\n\n\n\n\nweek 4\n\n\nhomework2\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nAnika Misra\n\n\n\n\n\n\n\n\n\n\n\n\nUsing SQL Querying to compare different countries’ climate trends and also to compute year-long trends of temperature increases\n\n\n\n\n\n\nweek 3\n\n\nhomework1\n\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\nAnika Misra\n\n\n\n\n\n\n\n\n\n\n\n\nHW0\n\n\n\n\n\n\nweek 2\n\n\nhomework0\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\nAnika Misra\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nart\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nAnika Misra\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nAnika Misra\n\n\n\n\n\n\nNo matching items"
  }
]